{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIRIW Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset : Book Summary Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is a text file containing summaries of 3000 books. Each book summary ranges from 500-1000 words in length and is accompanied by a title and an index that starts at 0. The text file consists of a series of lines, with each line representing a single book summary and its associated title and index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting txt to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>Book_Name</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Drowned Wednesday</td>\n",
       "      <td>Drowned Wednesday is the first Trustee among ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The Lost Hero</td>\n",
       "      <td>As the book opens, Jason awakens on a school ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The Eyes of the Overworld</td>\n",
       "      <td>Cugel is easily persuaded by the merchant Fia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Magic's Promise</td>\n",
       "      <td>The book opens with Herald-Mage Vanyel return...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Taran Wanderer</td>\n",
       "      <td>Taran and Gurgi have returned to Caer Dallben...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Doc_ID                  Book_Name  \\\n",
       "0       0          Drowned Wednesday   \n",
       "1       1              The Lost Hero   \n",
       "2       2  The Eyes of the Overworld   \n",
       "3       3            Magic's Promise   \n",
       "4       4             Taran Wanderer   \n",
       "\n",
       "                                             Summary  \n",
       "0   Drowned Wednesday is the first Trustee among ...  \n",
       "1   As the book opens, Jason awakens on a school ...  \n",
       "2   Cugel is easily persuaded by the merchant Fia...  \n",
       "3   The book opens with Herald-Mage Vanyel return...  \n",
       "4   Taran and Gurgi have returned to Caer Dallben...  "
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"BookSummaryDataset.txt\",delimiter='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Number of words per document :  2717.5523333333335\n"
     ]
    }
   ],
   "source": [
    "avg=0\n",
    "for i in df[\"Summary\"]:\n",
    "    avg+=len(i)\n",
    "avg=avg/(len(df))\n",
    "print(\"Average Number of words per document : \",avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus after lowercasing : \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>Book_Name</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>drowned wednesday</td>\n",
       "      <td>drowned wednesday is the first trustee among ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the lost hero</td>\n",
       "      <td>as the book opens, jason awakens on a school ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the eyes of the overworld</td>\n",
       "      <td>cugel is easily persuaded by the merchant fia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>magic's promise</td>\n",
       "      <td>the book opens with herald-mage vanyel return...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>taran wanderer</td>\n",
       "      <td>taran and gurgi have returned to caer dallben...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Doc_ID                  Book_Name  \\\n",
       "0       0          drowned wednesday   \n",
       "1       1              the lost hero   \n",
       "2       2  the eyes of the overworld   \n",
       "3       3            magic's promise   \n",
       "4       4             taran wanderer   \n",
       "\n",
       "                                             Summary  \n",
       "0   drowned wednesday is the first trustee among ...  \n",
       "1   as the book opens, jason awakens on a school ...  \n",
       "2   cugel is easily persuaded by the merchant fia...  \n",
       "3   the book opens with herald-mage vanyel return...  \n",
       "4   taran and gurgi have returned to caer dallben...  "
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lowercasing(df):\n",
    "    df_lowercasing=df.copy()\n",
    "    df_lowercasing['Book_Name'] = df['Book_Name'].apply(str.lower)\n",
    "    df_lowercasing['Summary'] = df['Summary'].apply(str.lower)\n",
    "    return df_lowercasing\n",
    "df_lowercasing=lowercasing(df)\n",
    "print(\"Corpus after lowercasing : \\n\")\n",
    "df_lowercasing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus after tokenization : \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>Book_Name</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>drowned wednesday</td>\n",
       "      <td>[drowned, wednesday, is, the, first, trustee, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the lost hero</td>\n",
       "      <td>[as, the, book, opens, ,, jason, awakens, on, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the eyes of the overworld</td>\n",
       "      <td>[cugel, is, easily, persuaded, by, the, mercha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>magic's promise</td>\n",
       "      <td>[the, book, opens, with, herald, -, mage, vany...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>taran wanderer</td>\n",
       "      <td>[taran, and, gurgi, have, returned, to, caer, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Doc_ID                  Book_Name  \\\n",
       "0       0          drowned wednesday   \n",
       "1       1              the lost hero   \n",
       "2       2  the eyes of the overworld   \n",
       "3       3            magic's promise   \n",
       "4       4             taran wanderer   \n",
       "\n",
       "                                             Summary  \n",
       "0  [drowned, wednesday, is, the, first, trustee, ...  \n",
       "1  [as, the, book, opens, ,, jason, awakens, on, ...  \n",
       "2  [cugel, is, easily, persuaded, by, the, mercha...  \n",
       "3  [the, book, opens, with, herald, -, mage, vany...  \n",
       "4  [taran, and, gurgi, have, returned, to, caer, ...  "
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "def tokenization(df_lowercasing):\n",
    "    df_tokenized=df_lowercasing.copy()\n",
    "    corpus=df_tokenized[\"Summary\"].values\n",
    "    tokenized_summaries = []\n",
    "    for i in range(len(corpus)):\n",
    "        # use loc accessor to modify original DataFrame directly\n",
    "        tokens = WordPunctTokenizer().tokenize(corpus[i])\n",
    "        tokenized_summaries.append(tokens)\n",
    "    df_tokenized[\"Summary\"] = tokenized_summaries\n",
    "    return df_tokenized\n",
    "\n",
    "df_tokenized=tokenization(df_lowercasing)\n",
    "print(\"Corpus after tokenization : \\n\")\n",
    "df_tokenized.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus after removing punctuation : \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>Book_Name</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>drowned wednesday</td>\n",
       "      <td>[drowned, wednesday, is, the, first, trustee, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the lost hero</td>\n",
       "      <td>[as, the, book, opens, jason, awakens, on, a, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the eyes of the overworld</td>\n",
       "      <td>[cugel, is, easily, persuaded, by, the, mercha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>magic's promise</td>\n",
       "      <td>[the, book, opens, with, herald, mage, vanyel,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>taran wanderer</td>\n",
       "      <td>[taran, and, gurgi, have, returned, to, caer, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Doc_ID                  Book_Name  \\\n",
       "0       0          drowned wednesday   \n",
       "1       1              the lost hero   \n",
       "2       2  the eyes of the overworld   \n",
       "3       3            magic's promise   \n",
       "4       4             taran wanderer   \n",
       "\n",
       "                                             Summary  \n",
       "0  [drowned, wednesday, is, the, first, trustee, ...  \n",
       "1  [as, the, book, opens, jason, awakens, on, a, ...  \n",
       "2  [cugel, is, easily, persuaded, by, the, mercha...  \n",
       "3  [the, book, opens, with, herald, mage, vanyel,...  \n",
       "4  [taran, and, gurgi, have, returned, to, caer, ...  "
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "def remove_punctuation(df_tokenized):\n",
    "    df_punc=df_tokenized.copy()\n",
    "    translator =str.maketrans('', '', string.punctuation+\" \")\n",
    "    for i in range(len(df_tokenized['Summary'])):\n",
    "        df_punc['Summary'][i] = [token.translate(translator) for token in df_tokenized['Summary'][i]]\n",
    "        df_punc['Summary'][i] = list(filter(None, df_punc['Summary'][i])) #to remove space that got generated while removing punctuation\n",
    "\n",
    "    return df_punc\n",
    "df_punc=remove_punctuation(df_tokenized)\n",
    "print(\"Corpus after removing punctuation : \\n\")\n",
    "\n",
    "df_punc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of all stopwords in english : \n",
      " ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "Number of stopwords :  179\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(\"List of all stopwords in english : \\n\",stopwords.words('english'))\n",
    "print(\"Number of stopwords : \",len(stopwords.words('english')))\n",
    "\n",
    "\n",
    "def avg_words_per_document(corpus,length):\n",
    "    avg_words_per_doc=0\n",
    "    for i in corpus:\n",
    "        avg_words_per_doc+=len(i)\n",
    "    avg_words_per_doc=avg_words_per_doc/length\n",
    "    return avg_words_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words per document before stopword removal :  475.325\n"
     ]
    }
   ],
   "source": [
    "print(\"Average number of words per document before stopword removal : \",avg_words_per_document(df_punc['Summary'],3000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the concept of **multithreading** to remove stopwords to reduce the time taken to remove 179 stopwords from 3000 documents (each document containing around 500-1000 words)\n",
    "\n",
    "We run multiple threads of execution concurrently within a single program. A thread can be used to perform a specific task independently of the main thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus after removing stopwords : \n",
      "\n",
      "0    [drowned, wednesday, first, trustee, among, mo...\n",
      "1    [book, opens, jason, awakens, school, bus, una...\n",
      "2    [cugel, easily, persuaded, merchant, fianosthe...\n",
      "3    [book, opens, herald, mage, vanyel, returning,...\n",
      "4    [taran, gurgi, returned, caer, dallben, follow...\n",
      "Name: Summary, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "from functools import partial\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(document):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in document if word not in stop_words]\n",
    "\n",
    "def process_documents(documents):\n",
    "    threads = []\n",
    "    for i, document in enumerate(documents):\n",
    "        # create a partial function to call remove_stopwords with both i and document\n",
    "        target_func = partial(remove_stopwords, document) #(func,list)\n",
    "        thread = threading.Thread(target=lambda idx, func: documents.__setitem__(idx, func()), args=(i, target_func))\n",
    "        #documents.__setitem__(idx, func()) will modify the list at specified index idx, setting it to thte result \n",
    "        #of calling function\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "        \n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    return documents\n",
    "\n",
    "df_sw=df_punc.copy()\n",
    "corpus=df_sw['Summary']\n",
    "df_sw['Summary'] = process_documents(corpus)\n",
    "print(\"Corpus after removing stopwords : \\n\")\n",
    "\n",
    "print(df_sw['Summary'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus after lemmatization : \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>Book_Name</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>drowned wednesday</td>\n",
       "      <td>[drowned, wednesday, first, trustee, among, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the lost hero</td>\n",
       "      <td>[book, open, jason, awakens, school, bus, unab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the eyes of the overworld</td>\n",
       "      <td>[cugel, easily, persuaded, merchant, fianosthe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>magic's promise</td>\n",
       "      <td>[book, open, herald, mage, vanyel, returning, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>taran wanderer</td>\n",
       "      <td>[taran, gurgi, returned, caer, dallben, follow...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Doc_ID                  Book_Name  \\\n",
       "0       0          drowned wednesday   \n",
       "1       1              the lost hero   \n",
       "2       2  the eyes of the overworld   \n",
       "3       3            magic's promise   \n",
       "4       4             taran wanderer   \n",
       "\n",
       "                                             Summary  \n",
       "0  [drowned, wednesday, first, trustee, among, mo...  \n",
       "1  [book, open, jason, awakens, school, bus, unab...  \n",
       "2  [cugel, easily, persuaded, merchant, fianosthe...  \n",
       "3  [book, open, herald, mage, vanyel, returning, ...  \n",
       "4  [taran, gurgi, returned, caer, dallben, follow...  "
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatization_df(df_sw):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df_lemmaztized=df_sw.copy()\n",
    "    lem=[]\n",
    "    for i in range(len(df_lemmaztized[\"Summary\"])):\n",
    "        lem.append([lemmatizer.lemmatize(w) for w in df_sw[\"Summary\"][i]])\n",
    "    df_lemmaztized['Summary']=lem\n",
    "    return df_lemmaztized\n",
    "\n",
    "df_lemmaztized=lemmatization_df(df_sw)\n",
    "print(\"Corpus after lemmatization : \\n\")\n",
    "df_lemmaztized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus after stemming : \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>Book_Name</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>drowned wednesday</td>\n",
       "      <td>[drown, wednesday, first, truste, among, morro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the lost hero</td>\n",
       "      <td>[book, open, jason, awaken, school, bu, unabl,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the eyes of the overworld</td>\n",
       "      <td>[cugel, easili, persuad, merchant, fianosth, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>magic's promise</td>\n",
       "      <td>[book, open, herald, mage, vanyel, return, cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>taran wanderer</td>\n",
       "      <td>[taran, gurgi, return, caer, dallben, follow, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Doc_ID                  Book_Name  \\\n",
       "0       0          drowned wednesday   \n",
       "1       1              the lost hero   \n",
       "2       2  the eyes of the overworld   \n",
       "3       3            magic's promise   \n",
       "4       4             taran wanderer   \n",
       "\n",
       "                                             Summary  \n",
       "0  [drown, wednesday, first, truste, among, morro...  \n",
       "1  [book, open, jason, awaken, school, bu, unabl,...  \n",
       "2  [cugel, easili, persuad, merchant, fianosth, a...  \n",
       "3  [book, open, herald, mage, vanyel, return, cou...  \n",
       "4  [taran, gurgi, return, caer, dallben, follow, ...  "
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "def stemming(df_sw): \n",
    "    ps = PorterStemmer()\n",
    "    df_stemmed=df_sw.copy()\n",
    "    stem=[]\n",
    "    for i in range(len(df_stemmed[\"Summary\"])):\n",
    "            stem.append([ps.stem(w) for w in df_sw[\"Summary\"][i]])\n",
    "    df_stemmed['Summary']=stem\n",
    "    return df_stemmed\n",
    "\n",
    "df_stemmed=stemming(df_sw)\n",
    "print(\"Corpus after stemming : \\n\")\n",
    "df_stemmed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Preprocessed Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus after preprocessing : \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>Book_Name</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>drowned wednesday</td>\n",
       "      <td>drowned wednesday first trustee among morrow d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the lost hero</td>\n",
       "      <td>book open jason awakens school bus unable reme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the eyes of the overworld</td>\n",
       "      <td>cugel easily persuaded merchant fianosther att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>magic's promise</td>\n",
       "      <td>book open herald mage vanyel returning country...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>taran wanderer</td>\n",
       "      <td>taran gurgi returned caer dallben following ev...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Doc_ID                  Book_Name  \\\n",
       "0       0          drowned wednesday   \n",
       "1       1              the lost hero   \n",
       "2       2  the eyes of the overworld   \n",
       "3       3            magic's promise   \n",
       "4       4             taran wanderer   \n",
       "\n",
       "                                             Summary  \n",
       "0  drowned wednesday first trustee among morrow d...  \n",
       "1  book open jason awakens school bus unable reme...  \n",
       "2  cugel easily persuaded merchant fianosther att...  \n",
       "3  book open herald mage vanyel returning country...  \n",
       "4  taran gurgi returned caer dallben following ev...  "
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed=df_lemmaztized.copy()\n",
    "summary=[]\n",
    "for i in range(len(df_lemmaztized[\"Summary\"])):\n",
    "    summary.append(\" \".join(df_lemmaztized['Summary'][i]))\n",
    "    \n",
    "df_preprocessed[\"Summary\"]=summary\n",
    "print(\"Corpus after preprocessing : \\n\")\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We either perform lemmatization or stemming, we dont perform both one after another since it could lead to over normalization of terms !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **dictionary** for inverted index where the key will the term and the value will be a list of document IDs in which the term appears\n",
    "\n",
    "Dictionary : {Key (Term) : Value (Doc Ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size (Lemmatization) :  45309\n",
      "Vocabulary Size (Stemming) :  35766\n"
     ]
    }
   ],
   "source": [
    "def inverted_index(df):\n",
    "    inverted_index_dict={}\n",
    "    for doc_id in range(len(df)):\n",
    "        for term in df['Summary'][doc_id]:\n",
    "            if term not in inverted_index_dict:\n",
    "                inverted_index_dict[term]=[]\n",
    "            if doc_id not in inverted_index_dict[term]:\n",
    "                inverted_index_dict[term].append(doc_id)\n",
    "\n",
    "    return inverted_index_dict\n",
    "\n",
    "inverted_index_lemmatized=inverted_index(df_lemmaztized)\n",
    "inverted_index_stemmed=inverted_index(df_stemmed)\n",
    "print(\"Vocabulary Size (Lemmatization) : \", len(inverted_index_lemmatized))\n",
    "print(\"Vocabulary Size (Stemming) : \", len(inverted_index_stemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index on lemmatized data : \n",
      "drowned:\n",
      " [0, 172, 306, 312, 374, 376, 379, 424, 431, 659, 975, 1067, 1150, 1180, 1247, 1294, 1384, 1411, 1430, 1435, 1440, 1493, 1530, 1578, 1659, 1840, 1871, 2049, 2054, 2114, 2176, 2296, 2628, 2672, 2929]\n",
      "wednesday:\n",
      " [0, 1844, 2284]\n",
      "first:\n",
      " [0, 1, 4, 10, 14, 18, 19, 33, 34, 38, 41, 47, 50, 53, 54, 57, 60, 62, 66, 67, 69, 71, 73, 75, 76, 79, 80, 82, 83, 87, 88, 89, 101, 103, 106, 109, 112, 115, 118, 121, 124, 128, 130, 131, 133, 134, 139, 140, 147, 149, 151, 156, 157, 158, 159, 160, 161, 162, 163, 167, 168, 169, 170, 172, 175, 178, 181, 184, 185, 186, 188, 191, 192, 193, 194, 195, 196, 202, 205, 209, 214, 228, 229, 230, 232, 233, 236, 239, 240, 245, 248, 249, 250, 252, 257, 261, 269, 272, 276, 278, 279, 280, 284, 290, 293, 294, 296, 299, 306, 310, 311, 312, 313, 315, 316, 317, 318, 321, 323, 324, 327, 329, 334, 337, 341, 343, 344, 345, 349, 359, 361, 363, 365, 367, 368, 370, 372, 375, 377, 380, 385, 389, 391, 394, 395, 397, 398, 399, 408, 410, 414, 416, 417, 418, 419, 422, 423, 425, 427, 429, 430, 432, 433, 435, 436, 442, 444, 446, 447, 449, 451, 453, 454, 457, 458, 460, 471, 473, 475, 477, 478, 479, 481, 483, 486, 487, 488, 489, 490, 491, 497, 498, 499, 501, 502, 503, 505, 506, 508, 510, 511, 512, 513, 516, 521, 524, 528, 530, 531, 534, 536, 537, 538, 544, 545, 546, 555, 556, 557, 558, 559, 564, 569, 570, 572, 575, 576, 578, 579, 585, 587, 590, 594, 597, 601, 603, 604, 605, 606, 608, 613, 615, 618, 626, 627, 630, 631, 632, 634, 636, 637, 644, 646, 648, 651, 652, 658, 660, 664, 668, 670, 673, 677, 678, 681, 684, 687, 688, 689, 690, 691, 694, 697, 708, 709, 711, 714, 716, 718, 726, 727, 728, 730, 732, 736, 738, 739, 743, 745, 746, 750, 752, 754, 756, 764, 769, 770, 771, 773, 776, 777, 780, 781, 785, 789, 793, 804, 806, 812, 815, 819, 820, 822, 824, 825, 827, 829, 833, 842, 843, 844, 847, 848, 850, 852, 861, 873, 878, 884, 885, 888, 891, 893, 895, 896, 911, 914, 916, 923, 925, 928, 931, 932, 937, 939, 940, 945, 949, 951, 954, 955, 957, 959, 960, 961, 965, 969, 974, 975, 979, 982, 984, 986, 988, 989, 990, 992, 994, 995, 997, 1001, 1002, 1010, 1015, 1018, 1019, 1020, 1024, 1025, 1026, 1030, 1034, 1037, 1051, 1053, 1056, 1064, 1066, 1071, 1074, 1078, 1081, 1085, 1088, 1090, 1092, 1097, 1101, 1102, 1103, 1109, 1111, 1115, 1120, 1121, 1126, 1135, 1138, 1140, 1142, 1149, 1150, 1152, 1158, 1159, 1162, 1164, 1167, 1172, 1173, 1174, 1175, 1176, 1177, 1180, 1185, 1187, 1194, 1197, 1200, 1202, 1206, 1207, 1208, 1209, 1211, 1213, 1215, 1219, 1220, 1221, 1222, 1230, 1232, 1243, 1247, 1251, 1254, 1261, 1263, 1265, 1268, 1269, 1272, 1275, 1276, 1277, 1278, 1282, 1288, 1290, 1306, 1309, 1314, 1315, 1323, 1328, 1330, 1339, 1340, 1345, 1346, 1351, 1352, 1356, 1358, 1365, 1368, 1370, 1374, 1380, 1383, 1384, 1386, 1394, 1403, 1409, 1412, 1413, 1415, 1418, 1422, 1434, 1435, 1436, 1440, 1441, 1445, 1446, 1455, 1458, 1460, 1461, 1462, 1463, 1469, 1473, 1477, 1479, 1482, 1489, 1492, 1499, 1502, 1504, 1513, 1516, 1522, 1524, 1528, 1530, 1535, 1536, 1539, 1541, 1550, 1552, 1556, 1559, 1560, 1561, 1565, 1566, 1569, 1573, 1575, 1576, 1577, 1578, 1581, 1585, 1586, 1596, 1597, 1601, 1606, 1607, 1608, 1610, 1614, 1618, 1620, 1634, 1638, 1642, 1644, 1645, 1647, 1648, 1653, 1656, 1658, 1661, 1662, 1663, 1664, 1667, 1669, 1670, 1672, 1676, 1682, 1687, 1688, 1689, 1692, 1693, 1694, 1696, 1699, 1700, 1702, 1703, 1705, 1707, 1708, 1711, 1712, 1715, 1723, 1726, 1727, 1728, 1730, 1732, 1734, 1741, 1745, 1746, 1747, 1750, 1752, 1759, 1760, 1769, 1771, 1774, 1777, 1784, 1787, 1793, 1795, 1796, 1798, 1802, 1804, 1805, 1807, 1811, 1812, 1813, 1815, 1816, 1817, 1818, 1820, 1821, 1822, 1824, 1829, 1832, 1835, 1838, 1840, 1842, 1845, 1848, 1853, 1857, 1860, 1862, 1864, 1869, 1870, 1871, 1874, 1878, 1879, 1881, 1882, 1885, 1887, 1890, 1892, 1893, 1894, 1898, 1899, 1902, 1904, 1905, 1908, 1912, 1913, 1917, 1919, 1920, 1921, 1922, 1925, 1926, 1927, 1928, 1937, 1940, 1942, 1945, 1949, 1950, 1952, 1953, 1956, 1957, 1958, 1960, 1966, 1967, 1968, 1971, 1972, 1976, 1977, 1978, 1980, 1982, 1983, 1984, 1986, 1990, 1993, 1995, 1997, 1999, 2000, 2001, 2002, 2003, 2004, 2006, 2011, 2013, 2023, 2026, 2030, 2031, 2033, 2037, 2038, 2052, 2054, 2058, 2060, 2070, 2071, 2072, 2074, 2075, 2076, 2081, 2085, 2086, 2088, 2089, 2092, 2093, 2094, 2097, 2099, 2104, 2110, 2114, 2117, 2118, 2125, 2129, 2133, 2134, 2144, 2147, 2148, 2149, 2150, 2152, 2155, 2159, 2162, 2167, 2168, 2169, 2170, 2178, 2179, 2180, 2181, 2182, 2186, 2192, 2201, 2206, 2207, 2212, 2214, 2216, 2219, 2220, 2221, 2224, 2228, 2233, 2234, 2242, 2247, 2250, 2251, 2252, 2258, 2259, 2261, 2265, 2271, 2272, 2276, 2280, 2281, 2282, 2293, 2296, 2298, 2303, 2305, 2313, 2318, 2321, 2323, 2333, 2336, 2337, 2338, 2339, 2342, 2346, 2349, 2350, 2351, 2352, 2357, 2359, 2364, 2370, 2371, 2374, 2376, 2382, 2383, 2386, 2388, 2391, 2392, 2396, 2398, 2399, 2405, 2408, 2422, 2424, 2432, 2436, 2439, 2440, 2443, 2447, 2448, 2449, 2459, 2461, 2462, 2470, 2471, 2476, 2477, 2498, 2500, 2502, 2505, 2511, 2517, 2519, 2522, 2525, 2531, 2537, 2541, 2548, 2551, 2552, 2559, 2561, 2564, 2567, 2570, 2572, 2575, 2577, 2578, 2579, 2580, 2582, 2587, 2588, 2590, 2592, 2601, 2606, 2609, 2612, 2615, 2617, 2625, 2627, 2629, 2635, 2636, 2639, 2642, 2644, 2647, 2650, 2656, 2657, 2659, 2661, 2663, 2674, 2677, 2679, 2686, 2689, 2691, 2692, 2694, 2697, 2699, 2710, 2714, 2716, 2717, 2718, 2720, 2723, 2733, 2734, 2737, 2739, 2740, 2741, 2742, 2744, 2747, 2749, 2751, 2752, 2753, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2764, 2767, 2772, 2773, 2775, 2778, 2784, 2785, 2792, 2793, 2794, 2795, 2796, 2797, 2810, 2814, 2817, 2826, 2827, 2829, 2830, 2843, 2845, 2849, 2850, 2854, 2855, 2860, 2863, 2864, 2873, 2875, 2880, 2881, 2884, 2885, 2886, 2893, 2897, 2899, 2902, 2908, 2910, 2911, 2921, 2923, 2925, 2929, 2930, 2934, 2938, 2940, 2943, 2945, 2950, 2952, 2955, 2957, 2963, 2964, 2966, 2968, 2969, 2975, 2977, 2979, 2982, 2986, 2989, 2999]\n",
      "trustee:\n",
      " [0, 444, 537, 556, 1033, 1215, 1306, 1862, 1874, 2785]\n"
     ]
    }
   ],
   "source": [
    "print(\"Inverted index on lemmatized data : \")\n",
    "for key in list(inverted_index_lemmatized.keys())[:4]:\n",
    "    print(f\"{key}:\\n {inverted_index_lemmatized[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index on stemmed data : \n",
      "drowned:\n",
      " [0, 172, 306, 312, 374, 376, 379, 424, 431, 659, 975, 1067, 1150, 1180, 1247, 1294, 1384, 1411, 1430, 1435, 1440, 1493, 1530, 1578, 1659, 1840, 1871, 2049, 2054, 2114, 2176, 2296, 2628, 2672, 2929]\n",
      "wednesday:\n",
      " [0, 1844, 2284]\n",
      "first:\n",
      " [0, 1, 4, 10, 14, 18, 19, 33, 34, 38, 41, 47, 50, 53, 54, 57, 60, 62, 66, 67, 69, 71, 73, 75, 76, 79, 80, 82, 83, 87, 88, 89, 101, 103, 106, 109, 112, 115, 118, 121, 124, 128, 130, 131, 133, 134, 139, 140, 147, 149, 151, 156, 157, 158, 159, 160, 161, 162, 163, 167, 168, 169, 170, 172, 175, 178, 181, 184, 185, 186, 188, 191, 192, 193, 194, 195, 196, 202, 205, 209, 214, 228, 229, 230, 232, 233, 236, 239, 240, 245, 248, 249, 250, 252, 257, 261, 269, 272, 276, 278, 279, 280, 284, 290, 293, 294, 296, 299, 306, 310, 311, 312, 313, 315, 316, 317, 318, 321, 323, 324, 327, 329, 334, 337, 341, 343, 344, 345, 349, 359, 361, 363, 365, 367, 368, 370, 372, 375, 377, 380, 385, 389, 391, 394, 395, 397, 398, 399, 408, 410, 414, 416, 417, 418, 419, 422, 423, 425, 427, 429, 430, 432, 433, 435, 436, 442, 444, 446, 447, 449, 451, 453, 454, 457, 458, 460, 471, 473, 475, 477, 478, 479, 481, 483, 486, 487, 488, 489, 490, 491, 497, 498, 499, 501, 502, 503, 505, 506, 508, 510, 511, 512, 513, 516, 521, 524, 528, 530, 531, 534, 536, 537, 538, 544, 545, 546, 555, 556, 557, 558, 559, 564, 569, 570, 572, 575, 576, 578, 579, 585, 587, 590, 594, 597, 601, 603, 604, 605, 606, 608, 613, 615, 618, 626, 627, 630, 631, 632, 634, 636, 637, 644, 646, 648, 651, 652, 658, 660, 664, 668, 670, 673, 677, 678, 681, 684, 687, 688, 689, 690, 691, 694, 697, 708, 709, 711, 714, 716, 718, 726, 727, 728, 730, 732, 736, 738, 739, 743, 745, 746, 750, 752, 754, 756, 764, 769, 770, 771, 773, 776, 777, 780, 781, 785, 789, 793, 804, 806, 812, 815, 819, 820, 822, 824, 825, 827, 829, 833, 842, 843, 844, 847, 848, 850, 852, 861, 873, 878, 884, 885, 888, 891, 893, 895, 896, 911, 914, 916, 923, 925, 928, 931, 932, 937, 939, 940, 945, 949, 951, 954, 955, 957, 959, 960, 961, 965, 969, 974, 975, 979, 982, 984, 986, 988, 989, 990, 992, 994, 995, 997, 1001, 1002, 1010, 1015, 1018, 1019, 1020, 1024, 1025, 1026, 1030, 1034, 1037, 1051, 1053, 1056, 1064, 1066, 1071, 1074, 1078, 1081, 1085, 1088, 1090, 1092, 1097, 1101, 1102, 1103, 1109, 1111, 1115, 1120, 1121, 1126, 1135, 1138, 1140, 1142, 1149, 1150, 1152, 1158, 1159, 1162, 1164, 1167, 1172, 1173, 1174, 1175, 1176, 1177, 1180, 1185, 1187, 1194, 1197, 1200, 1202, 1206, 1207, 1208, 1209, 1211, 1213, 1215, 1219, 1220, 1221, 1222, 1230, 1232, 1243, 1247, 1251, 1254, 1261, 1263, 1265, 1268, 1269, 1272, 1275, 1276, 1277, 1278, 1282, 1288, 1290, 1306, 1309, 1314, 1315, 1323, 1328, 1330, 1339, 1340, 1345, 1346, 1351, 1352, 1356, 1358, 1365, 1368, 1370, 1374, 1380, 1383, 1384, 1386, 1394, 1403, 1409, 1412, 1413, 1415, 1418, 1422, 1434, 1435, 1436, 1440, 1441, 1445, 1446, 1455, 1458, 1460, 1461, 1462, 1463, 1469, 1473, 1477, 1479, 1482, 1489, 1492, 1499, 1502, 1504, 1513, 1516, 1522, 1524, 1528, 1530, 1535, 1536, 1539, 1541, 1550, 1552, 1556, 1559, 1560, 1561, 1565, 1566, 1569, 1573, 1575, 1576, 1577, 1578, 1581, 1585, 1586, 1596, 1597, 1601, 1606, 1607, 1608, 1610, 1614, 1618, 1620, 1634, 1638, 1642, 1644, 1645, 1647, 1648, 1653, 1656, 1658, 1661, 1662, 1663, 1664, 1667, 1669, 1670, 1672, 1676, 1682, 1687, 1688, 1689, 1692, 1693, 1694, 1696, 1699, 1700, 1702, 1703, 1705, 1707, 1708, 1711, 1712, 1715, 1723, 1726, 1727, 1728, 1730, 1732, 1734, 1741, 1745, 1746, 1747, 1750, 1752, 1759, 1760, 1769, 1771, 1774, 1777, 1784, 1787, 1793, 1795, 1796, 1798, 1802, 1804, 1805, 1807, 1811, 1812, 1813, 1815, 1816, 1817, 1818, 1820, 1821, 1822, 1824, 1829, 1832, 1835, 1838, 1840, 1842, 1845, 1848, 1853, 1857, 1860, 1862, 1864, 1869, 1870, 1871, 1874, 1878, 1879, 1881, 1882, 1885, 1887, 1890, 1892, 1893, 1894, 1898, 1899, 1902, 1904, 1905, 1908, 1912, 1913, 1917, 1919, 1920, 1921, 1922, 1925, 1926, 1927, 1928, 1937, 1940, 1942, 1945, 1949, 1950, 1952, 1953, 1956, 1957, 1958, 1960, 1966, 1967, 1968, 1971, 1972, 1976, 1977, 1978, 1980, 1982, 1983, 1984, 1986, 1990, 1993, 1995, 1997, 1999, 2000, 2001, 2002, 2003, 2004, 2006, 2011, 2013, 2023, 2026, 2030, 2031, 2033, 2037, 2038, 2052, 2054, 2058, 2060, 2070, 2071, 2072, 2074, 2075, 2076, 2081, 2085, 2086, 2088, 2089, 2092, 2093, 2094, 2097, 2099, 2104, 2110, 2114, 2117, 2118, 2125, 2129, 2133, 2134, 2144, 2147, 2148, 2149, 2150, 2152, 2155, 2159, 2162, 2167, 2168, 2169, 2170, 2178, 2179, 2180, 2181, 2182, 2186, 2192, 2201, 2206, 2207, 2212, 2214, 2216, 2219, 2220, 2221, 2224, 2228, 2233, 2234, 2242, 2247, 2250, 2251, 2252, 2258, 2259, 2261, 2265, 2271, 2272, 2276, 2280, 2281, 2282, 2293, 2296, 2298, 2303, 2305, 2313, 2318, 2321, 2323, 2333, 2336, 2337, 2338, 2339, 2342, 2346, 2349, 2350, 2351, 2352, 2357, 2359, 2364, 2370, 2371, 2374, 2376, 2382, 2383, 2386, 2388, 2391, 2392, 2396, 2398, 2399, 2405, 2408, 2422, 2424, 2432, 2436, 2439, 2440, 2443, 2447, 2448, 2449, 2459, 2461, 2462, 2470, 2471, 2476, 2477, 2498, 2500, 2502, 2505, 2511, 2517, 2519, 2522, 2525, 2531, 2537, 2541, 2548, 2551, 2552, 2559, 2561, 2564, 2567, 2570, 2572, 2575, 2577, 2578, 2579, 2580, 2582, 2587, 2588, 2590, 2592, 2601, 2606, 2609, 2612, 2615, 2617, 2625, 2627, 2629, 2635, 2636, 2639, 2642, 2644, 2647, 2650, 2656, 2657, 2659, 2661, 2663, 2674, 2677, 2679, 2686, 2689, 2691, 2692, 2694, 2697, 2699, 2710, 2714, 2716, 2717, 2718, 2720, 2723, 2733, 2734, 2737, 2739, 2740, 2741, 2742, 2744, 2747, 2749, 2751, 2752, 2753, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2764, 2767, 2772, 2773, 2775, 2778, 2784, 2785, 2792, 2793, 2794, 2795, 2796, 2797, 2810, 2814, 2817, 2826, 2827, 2829, 2830, 2843, 2845, 2849, 2850, 2854, 2855, 2860, 2863, 2864, 2873, 2875, 2880, 2881, 2884, 2885, 2886, 2893, 2897, 2899, 2902, 2908, 2910, 2911, 2921, 2923, 2925, 2929, 2930, 2934, 2938, 2940, 2943, 2945, 2950, 2952, 2955, 2957, 2963, 2964, 2966, 2968, 2969, 2975, 2977, 2979, 2982, 2986, 2989, 2999]\n",
      "trustee:\n",
      " [0, 444, 537, 556, 1033, 1215, 1306, 1862, 1874, 2785]\n"
     ]
    }
   ],
   "source": [
    "print(\"Inverted index on stemmed data : \")\n",
    "for key in list(inverted_index_lemmatized.keys())[:4]:\n",
    "    print(f\"{key}:\\n {inverted_index_lemmatized[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Query Processing With An Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Query processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import string\n",
    " \n",
    "def preprocess_query(query):\n",
    "    #lowercasing\n",
    "    query=query.lower()\n",
    "\n",
    "    #tokenization\n",
    "    query=wordpunct_tokenize(query)\n",
    "    \n",
    "    #removing punctuation\n",
    "    translator =str.maketrans('', '', string.punctuation+\" \")\n",
    "    query=[token.translate(translator) for token in query]\n",
    "    query=list(filter(None, query))\n",
    "\n",
    "    #stopword removal except and,or,not\n",
    "    stop_words = set(stopwords.words('english'))-{'and','or','not'}\n",
    "    query = [word for word in query if word not in stop_words]\n",
    "\n",
    "    #lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_query_terms=[lemmatizer.lemmatize(w) for w in query if w not in ['AND','OR','NOT']]\n",
    "\n",
    "    #stemming\n",
    "    ps=PorterStemmer()\n",
    "    stemmed_query_terms=[ps.stem(w) for w in query if w not in ['and','or','not']]\n",
    "\n",
    "    return lemmatized_query_terms,stemmed_query_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting processed query (infix) to postfix expression and evaluating postfix expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infix_to_postfix(expr):\n",
    "    prec = {\"not\": 3, \"and\": 2, \"or\": 1}        #operator precedence\n",
    "    output = []         #output stack\n",
    "    op_stack = []       #operator stack\n",
    "    tokens = expr\n",
    "    for token in tokens:\n",
    "        if token in prec:\n",
    "            # If the token is an operator, pop operators from the operator stack and append them to the output\n",
    "            # until the stack is empty or the top operator has lower precedence than the current operator.\n",
    "            while op_stack and op_stack[-1] in prec and prec[op_stack[-1]] >= prec[token]:\n",
    "                output.append(op_stack.pop())\n",
    "            op_stack.append(token)\n",
    "        else:\n",
    "            # If the token is an operand, append it to the output.\n",
    "            output.append(token)\n",
    "    # Pop any remaining operators from the operator stack and append them to the output\n",
    "    while op_stack:\n",
    "        output.append(op_stack.pop())\n",
    "    # Join the output list into a string and return it\n",
    "    return output\n",
    "\n",
    "def eval_postfix(lst):\n",
    "    stack = []\n",
    "    for token in lst:\n",
    "        if isinstance(token, list):\n",
    "            # If the token is an operand, push it onto the stack\n",
    "            stack.append(token)\n",
    "        elif token == \"not\":\n",
    "            # If the token is \"not\", pop the top operand from the stack, negate it, and push the result back onto the stack\n",
    "            operand = stack.pop()\n",
    "            # result = [x for x in range(len(df))]-operand\n",
    "            corpus=[x for x in range(3000)]\n",
    "            result=[x for x in corpus if x not in operand]\n",
    "            result.sort()\n",
    "            stack.append(result)\n",
    "        elif token == \"and\":\n",
    "            # If the token is \"and\", pop the top two operands from the stack, perform a logical \"and\" operation, and push the result back onto the stack\n",
    "            operand2 = stack.pop()\n",
    "            operand1 = stack.pop()\n",
    "            # print(\"Operands : \",operand1,operand2)\n",
    "            result = [value for value in operand1 if value in operand2]\n",
    "            result.sort()\n",
    "            stack.append(result)\n",
    "        elif token == \"or\":\n",
    "            # If the token is \"or\", pop the top two operands from the stack, perform a logical \"or\" operation, and push the result back onto the stack\n",
    "            operand2 = stack.pop()\n",
    "            operand1 = stack.pop()\n",
    "            result = list(set(operand1) | set(operand2))\n",
    "            result.sort()\n",
    "            stack.append(result)\n",
    "        else:\n",
    "            # If the token is an unknown operator or operand, raise an error\n",
    "            raise ValueError(f\"Unknown token {token}\")\n",
    "    # The final result is the only element left on the stack\n",
    "    return stack.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_query_processing_using_inverted_index(query):\n",
    "    #preprocessing the query\n",
    "    print(\"QUERY : \",query)\n",
    "    query_lemmatized,query_stemmed=preprocess_query(query)\n",
    "\n",
    "    #converting infix query to postfix query\n",
    "    postfix_expr=infix_to_postfix(query_lemmatized)\n",
    "\n",
    "    #mapping terms to their inverted indices\n",
    "    inverted_list_terms=[]\n",
    "    for i in postfix_expr:\n",
    "        if i not in ['and','or','not']:\n",
    "            if i not in inverted_index_lemmatized:\n",
    "                return f\"Word \\\"{i}\\\" not found in corpus\"\n",
    "            inverted_list_terms.append(inverted_index_lemmatized[i])\n",
    "        else:\n",
    "            inverted_list_terms.append(i)\n",
    "\n",
    "    #Evaluating the postfix expression to fetch the required documents\n",
    "    retreived_documents=eval_postfix(inverted_list_terms)\n",
    "    print(\"Retreived Documents : \",retreived_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY :  journey and mountain and rocks\n",
      "Retreived Documents :  [244, 410, 590, 2150, 2244, 2405]\n"
     ]
    }
   ],
   "source": [
    "boolean_query_processing_using_inverted_index(\"journey and mountain and rocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY :  monitoring and not moon\n",
      "Retreived Documents :  [217, 611, 672, 1019, 2626, 2661, 2809, 2940]\n"
     ]
    }
   ],
   "source": [
    "boolean_query_processing_using_inverted_index(\"monitoring and not moon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY :  Tuscany or carpenter\n",
      "Retreived Documents :  [29, 54, 345, 745, 1037, 1251, 1263, 1282, 1360, 1583, 1630, 1681, 1727, 1871, 1890, 1908, 2196, 2301, 2840, 2873, 2934, 2969]\n"
     ]
    }
   ],
   "source": [
    "boolean_query_processing_using_inverted_index(\"Tuscany or carpenter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Query \n",
    "* Biword Index\n",
    "* Extended Biwords index (NXN Biwords)\n",
    "\n",
    "Biword indexing and extended biword indexing are methods of implementation of phrase query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biword index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most frequent Biwords :\n",
      "  [('year old', 410), ('next day', 273), ('new york', 272), ('take place', 230), ('united state', 182), ('one day', 171), ('jean claude', 155), ('fall love', 152), ('year later', 150), ('year ago', 139)]\n"
     ]
    }
   ],
   "source": [
    "def biword_index(df):\n",
    "    biword_dict = {}\n",
    "    biword_count={}\n",
    "\n",
    "    for doc in range(len(df)):\n",
    "        words = df['Summary'][doc]\n",
    "        \n",
    "        # Create bi-words by pairing adjacent words\n",
    "        biwords = [words[i] + \" \" + words[i+1] for i in range(len(words)-1)]\n",
    "        \n",
    "        # Add each bi-word to the index\n",
    "        for biword in biwords:\n",
    "            if biword in biword_dict:\n",
    "                biword_count[biword]+=1\n",
    "                if doc not in biword_dict[biword]:\n",
    "                    biword_dict[biword].append(doc)\n",
    "            else:\n",
    "                biword_dict[biword] = [doc]\n",
    "                biword_count[biword]=1\n",
    "\n",
    "    sorted_biword_count = dict(sorted(biword_count.items(), key=lambda item: item[1], reverse=True))\n",
    "    print(\"Top 10 Most frequent Biwords :\\n \",list(sorted_biword_count.items())[:10])\n",
    "\n",
    "    return biword_dict,biword_count,sorted_biword_count\n",
    "\n",
    "biword_dict,biword_count,sorted_biword_count=biword_index(df_lemmaztized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Biwords :  617573\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Number of Biwords : \",len(biword_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended Biwords (NX*N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger: [WinError 10054]\n",
      "[nltk_data]     An existing connection was forcibly closed by the\n",
      "[nltk_data]     remote host\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import concurrent.futures\n",
    "\n",
    "def generate_extended_biwords(df):    \n",
    "    extended_biwords = []\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for doc in range(len(df)):\n",
    "            futures.append(executor.submit(process_document, df['Summary'][doc]))\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            extended_biwords.extend(future.result())\n",
    "    return extended_biwords\n",
    "\n",
    "def process_document(doc):\n",
    "    tagged_words = nltk.pos_tag(doc)\n",
    "    extended_biwords = []\n",
    "    for i, tagged_word in enumerate(tagged_words):\n",
    "        word, tag = tagged_word\n",
    "        # If the word is a noun ->add it to the extended biword\n",
    "        if tag.startswith('N'):\n",
    "            extended_biword = word\n",
    "            # Look for the next noun or the end of the list\n",
    "            for j in range(i+1, len(tagged_words)):\n",
    "                next_word, next_tag = tagged_words[j]\n",
    "                # If the next word is a noun ->add it to the extended biword\n",
    "                if next_tag.startswith('N'):\n",
    "                    extended_biword += \" \" + next_word\n",
    "                    extended_biwords.append(extended_biword)\n",
    "                    break\n",
    "                # If the next word is an article or preposition ->add it to the extended biword\n",
    "                elif next_tag.startswith('D') or next_tag.startswith('I'):\n",
    "                    extended_biword += \" \" + next_word\n",
    "                # If the next word is not a noun, article, or preposition ->stop looking ahead\n",
    "                else:\n",
    "                    break\n",
    "    return extended_biwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_biwords = generate_extended_biwords(df_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few of the extended biwords : \n",
      " ['release date', 'concerns the dwelling', 'order of the renunciates', 'house in exchange', 'amazon jaelle', 'wife of an earthman', 'merchant fianosther', 'burglary of the manse', 'manse of iucounu', 'hemisphere of violet', 'violet glass', 'glass an eye', 'eye of the overworld', 'wizard s', 's possession', 'entity of barbs', 'loyalty zeal', 'singleness of purpose', 'land of cutz', 'wearers of the violet']\n"
     ]
    }
   ],
   "source": [
    "print(\"Few of the extended biwords : \\n\",extended_biwords[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Considering NXN biwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of extended biwords :  145510\n",
      "\n",
      "Number of NN :  84724\n",
      "Number of NXN :  34264\n",
      "Number of NXXN :  25929\n",
      "Number of NXXXN :  538\n",
      "\n",
      "Few NN extended biwords :  ['release date', 'amazon jaelle', 'merchant fianosther', 'violet glass', 'wizard s']\n",
      "Few NXN extended biwords :  ['concerns the dwelling', 'house in exchange', 'manse of iucounu', 'hemisphere of violet', 'glass an eye']\n",
      "Few NXXN extended biwords :  ['order of the renunciates', 'wife of an earthman', 'burglary of the manse', 'eye of the overworld', 'wearers of the violet']\n",
      "Few NXXXN extended biwords :  ['wears thin with the seal', 'cross over into the realms', 'incantation so that the beast', 'dart out of a tree', 'none of this the prism']\n"
     ]
    }
   ],
   "source": [
    "NN=[i for i in extended_biwords if len(i.split())==2]\n",
    "NXN=[i for i in extended_biwords if len(i.split())==3]\n",
    "NXXN=[i for i in extended_biwords if len(i.split())==4]\n",
    "NXXXN=[i for i in extended_biwords if len(i.split())==5]\n",
    "\n",
    "print(\"Total number of extended biwords : \",len(extended_biwords))\n",
    "print(\"\\nNumber of NN : \",len(NN))\n",
    "print(\"Number of NXN : \",len(NXN))\n",
    "print(\"Number of NXXN : \",len(NXXN))\n",
    "print(\"Number of NXXXN : \",len(NXXXN))\n",
    "\n",
    "print(\"\\nFew NN extended biwords : \",NN[:5])\n",
    "print(\"Few NXN extended biwords : \",NXN[:5])\n",
    "print(\"Few NXXN extended biwords : \",NXXN[:5])\n",
    "print(\"Few NXXXN extended biwords : \",NXXXN[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented using intesect algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_positional_index(documents):\n",
    "    index = {}\n",
    "    for doc_id, document in enumerate(documents):\n",
    "        for position, token in enumerate(document):\n",
    "            if token not in index:\n",
    "                index[token] = {}\n",
    "            if doc_id not in index[token]:\n",
    "                index[token][doc_id] = []\n",
    "            index[token][doc_id].append(position)\n",
    "    return index\n",
    "\n",
    "def preprocess_query_positional_index(query):#lemmatization, stemming , lowercasing, punctuation\n",
    "    #lowercasing\n",
    "    query=query.lower()\n",
    "\n",
    "    #tokenization\n",
    "    query=wordpunct_tokenize(query)\n",
    "    \n",
    "    #removing punctuation\n",
    "    translator =str.maketrans('', '', string.punctuation+\" \")\n",
    "    query=[token.translate(translator) for token in query]\n",
    "    query=list(filter(None, query))\n",
    "\n",
    "    #stopword removal except and,or,not\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    query = [word for word in query if word not in stop_words]\n",
    "\n",
    "    #lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_query_terms=[lemmatizer.lemmatize(w) for w in query ]\n",
    "\n",
    "    #stemming\n",
    "    ps=PorterStemmer()\n",
    "    stemmed_query_terms=[ps.stem(w) for w in query ]\n",
    "\n",
    "    return lemmatized_query_terms,stemmed_query_terms\n",
    "\n",
    "def search(index, query):\n",
    "    query_tokens = query\n",
    "    candidates = list(range(len(index[list(index.keys())[0]])))\n",
    "    # Iterate through each query token and intersect the candidate set with the set of documents\n",
    "    # that contain the token at the next position\n",
    "    for i, token in enumerate(query_tokens):\n",
    "        if token not in index:\n",
    "            return \"No documents retreived (Token not in corpus)\"\n",
    "        if i == 0:\n",
    "            candidates = list(index[token].keys())\n",
    "            # print(i,\" First Candidate : \",token)\n",
    "            # print(\"Doc IDs of first candidate : \\n\",candidates)\n",
    "        else:\n",
    "            new_candidates = list(index[token].keys())\n",
    "            candidates = list(set(candidates).intersection(set(new_candidates)))\n",
    "            for doc_id in candidates:\n",
    "                positions = index[token][doc_id]    #1. [1]\n",
    "                # print(\"Token : \",token,\"\\n Positions : \",positions)\n",
    "                s=0\n",
    "                for j in range(len(positions)):\n",
    "                    if (positions[j] - 1) in index[query_tokens[i-1]][doc_id]:\n",
    "                        s=1\n",
    "                if s==0:      \n",
    "                    candidates.remove(doc_id)\n",
    "                    break\n",
    "\n",
    "    # Sort the candidate set by the number of matches, and return the documents in descending order\n",
    "    # of the number of matches\n",
    "    candidate_scores = [(doc_id, sum([len(index[token][doc_id]) for token in query_tokens])) for doc_id in candidates]\n",
    "    candidate_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [doc_id for (doc_id, score) in candidate_scores]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Index of first 10 terms in the vocabulary : \n",
      "\n",
      "\n",
      "drowned\n",
      " {0: [0, 23, 282], 172: [69, 687], 306: [457], 312: [784], 374: [76, 257], 376: [3], 379: [243], 424: [136], 431: [188], 659: [25], 975: [109, 126, 141], 1067: [40], 1150: [109, 424, 585], 1180: [308], 1247: [168], 1294: [63], 1384: [130, 186], 1411: [404], 1430: [20, 27, 153], 1435: [84], 1440: [547, 628], 1493: [264], 1530: [145], 1578: [159], 1659: [218], 1840: [243], 1871: [1333], 2049: [172], 2054: [29, 271, 1959, 2507, 2658], 2114: [803], 2176: [41], 2296: [147, 287], 2628: [16], 2672: [95], 2929: [456, 715]}\n",
      "\n",
      "wednesday\n",
      " {0: [1, 24, 217, 222, 227, 234, 246, 283, 352, 412], 1844: [257], 2284: [83, 87, 93, 99, 104, 184, 329, 374, 384, 422, 624, 628, 639]}\n",
      "\n",
      "first\n",
      " {0: [2, 114, 203, 382], 1: [166], 4: [56, 270, 529], 10: [67, 140], 14: [17, 508], 18: [154, 374], 19: [9], 33: [68], 34: [230], 38: [484], 41: [154], 47: [2], 50: [69, 378], 53: [65, 82], 54: [17, 47, 138], 57: [113], 60: [227, 405], 62: [228], 66: [34, 53], 67: [19, 77], 69: [144, 339], 71: [183], 73: [137], 75: [380, 421], 76: [235], 79: [44, 231], 80: [64], 82: [178, 257], 83: [239], 87: [31], 88: [566, 574, 642, 735, 825], 89: [3], 101: [195, 248], 103: [355], 106: [103, 128, 196], 109: [279], 112: [826], 115: [18], 118: [163], 121: [542], 124: [190], 128: [0, 65], 130: [27, 45], 131: [5], 133: [234], 134: [86], 139: [64, 178], 140: [47], 147: [238], 149: [3], 151: [429, 464], 156: [2], 157: [27, 56, 59, 75], 158: [9, 169], 159: [244], 160: [89], 161: [23, 31], 162: [248, 403, 462], 163: [184], 167: [77], 168: [395], 169: [62], 170: [447], 172: [340, 786], 175: [90], 178: [99, 260], 181: [399], 184: [434], 185: [8], 186: [44], 188: [162, 167], 191: [240, 660, 1430, 1554], 192: [9], 193: [245, 515, 1022, 1221, 1268], 194: [621], 195: [134, 216], 196: [30, 69, 236], 202: [181], 205: [96], 209: [4, 24, 205, 243], 214: [934], 228: [19], 229: [94], 230: [306], 232: [9, 287], 233: [9], 236: [64, 191], 239: [298], 240: [247], 245: [4, 32], 248: [42], 249: [41], 250: [308, 378], 252: [61], 257: [2, 18, 31, 73], 261: [38, 222], 269: [20], 272: [8], 276: [118], 278: [64, 85, 158, 200], 279: [911], 280: [308], 284: [77], 290: [28], 293: [96], 294: [142, 182], 296: [494, 523], 299: [97], 306: [202], 310: [198, 243, 277, 408, 887, 1182, 1347], 311: [280, 420, 611, 821], 312: [331], 313: [42], 315: [76], 316: [68], 317: [82, 89], 318: [0], 321: [100, 284], 323: [75, 103], 324: [50], 327: [547], 329: [131], 334: [332, 483, 504], 337: [57], 341: [57, 100, 1078], 343: [83], 344: [129], 345: [363], 349: [40, 245], 359: [43], 361: [417], 363: [2], 365: [167], 367: [277], 368: [158], 370: [36, 40], 372: [172], 375: [7], 377: [53, 58], 380: [291, 346, 376, 491, 515, 526], 385: [8], 389: [16], 391: [48, 59], 394: [32], 395: [371], 397: [216, 431], 398: [10], 399: [82], 408: [227, 289], 410: [139, 422, 831, 1140], 414: [543], 416: [126, 164], 417: [83, 446], 418: [8], 419: [131], 422: [62], 423: [164, 601], 425: [21, 331, 575], 427: [234], 429: [81, 163, 366], 430: [219, 928], 432: [146, 184], 433: [78], 435: [358], 436: [4, 116], 442: [53], 444: [100], 446: [53], 447: [16, 110, 327, 405], 449: [172], 451: [369, 506], 453: [263], 454: [15], 457: [12, 60, 69], 458: [38, 247], 460: [2], 471: [136], 473: [388], 475: [10], 477: [174], 478: [231], 479: [538], 481: [1, 60, 379, 610, 860, 877, 1404], 483: [504, 667], 486: [315], 487: [13], 488: [45], 489: [8], 490: [217], 491: [80], 497: [175], 498: [803], 499: [33], 501: [121], 502: [8], 503: [194, 197, 213], 505: [17], 506: [189], 508: [8], 510: [272], 511: [222, 456], 512: [199, 542], 513: [171, 308, 448, 457], 516: [20, 48], 521: [6, 1224], 524: [2], 528: [7], 530: [55], 531: [84], 534: [7], 536: [248], 537: [162, 247], 538: [205, 218], 544: [337, 844], 545: [62], 546: [0, 3], 555: [57, 63], 556: [9, 94, 212, 229], 557: [50, 77, 214], 558: [461, 477, 534], 559: [139], 564: [201, 294], 569: [430], 570: [244], 572: [225], 575: [26, 44], 576: [32], 578: [37, 46, 55], 579: [269], 585: [61], 587: [27, 88, 409], 590: [65], 594: [160], 597: [70], 601: [15], 603: [56], 604: [86], 605: [44], 606: [77, 109], 608: [84, 186], 613: [79, 349], 615: [168], 618: [16, 399], 626: [158], 627: [222, 387, 412], 630: [215], 631: [16], 632: [75], 634: [29, 62], 636: [2], 637: [8], 644: [438], 646: [9], 648: [36], 651: [541], 652: [148], 658: [42, 178, 193], 660: [148], 664: [18], 668: [6], 670: [36], 673: [401], 677: [8], 678: [5, 28, 62, 115, 140, 147], 681: [5, 69], 684: [135, 883], 687: [160, 162], 688: [7], 689: [22, 34, 39, 53], 690: [51], 691: [47], 694: [72, 156], 697: [37, 43, 47], 708: [357], 709: [143], 711: [43], 714: [6, 353, 379], 716: [16], 718: [61], 726: [0, 42, 241, 289], 727: [5, 488], 728: [20, 42, 277], 730: [17], 732: [4], 736: [375, 394, 404, 503, 641], 738: [107, 170, 297], 739: [146], 743: [412], 745: [0, 89], 746: [143], 750: [1], 752: [12, 47], 754: [51], 756: [52, 251, 261], 764: [44], 769: [198], 770: [158], 771: [197, 398], 773: [72, 197], 776: [88, 129], 777: [16, 457], 780: [138], 781: [42], 785: [43], 789: [61, 65], 793: [5], 804: [15], 806: [189], 812: [557, 722, 996], 815: [327, 342], 819: [35], 820: [70], 822: [32], 824: [178], 825: [134, 142], 827: [187, 207], 829: [222], 833: [238], 842: [101, 112], 843: [41], 844: [4, 29, 448, 500, 506, 652, 665, 678, 701], 847: [118, 218], 848: [5], 850: [137, 165], 852: [16, 377, 935], 861: [58], 873: [105], 878: [126, 134], 884: [5], 885: [39, 54, 129], 888: [444], 891: [24, 370, 423], 893: [73, 139, 232], 895: [0, 65], 896: [5, 25, 136, 217], 911: [66, 168], 914: [49], 916: [40], 923: [155], 925: [301, 342], 928: [590], 931: [23, 131], 932: [191, 358], 937: [115], 939: [0, 392, 536, 690], 940: [47], 945: [12], 949: [178], 951: [775], 954: [86], 955: [43], 957: [240], 959: [286], 960: [2], 961: [18, 150], 965: [12], 969: [96], 974: [126, 166], 975: [207], 979: [46], 982: [340, 731], 984: [334], 986: [430], 988: [0], 989: [16, 151, 184, 273], 990: [169], 992: [102, 183, 427, 777, 781, 827], 994: [37], 995: [71], 997: [6, 99], 1001: [31, 351], 1002: [9, 65], 1010: [66, 140], 1015: [31], 1018: [4], 1019: [112], 1020: [155], 1024: [133], 1025: [32, 56], 1026: [269], 1030: [1], 1034: [66], 1037: [353], 1051: [39], 1053: [53], 1056: [2], 1064: [31], 1066: [12], 1071: [0, 56, 188], 1074: [139], 1078: [29], 1081: [101, 150, 369], 1085: [97, 174], 1088: [36], 1090: [282, 397], 1092: [5], 1097: [170, 276], 1101: [153], 1102: [11], 1103: [2], 1109: [19, 57], 1111: [6], 1115: [943], 1120: [288], 1121: [185], 1126: [10], 1135: [253], 1138: [67, 116], 1140: [100, 346, 377, 408, 516, 810], 1142: [49], 1149: [9, 59, 78, 222, 292], 1150: [285, 335], 1152: [9], 1158: [56], 1159: [34], 1162: [614], 1164: [501, 523], 1167: [20], 1172: [11], 1173: [51], 1174: [4, 33, 255], 1175: [212], 1176: [109], 1177: [183], 1180: [295], 1185: [295, 666, 1057], 1187: [22], 1194: [4], 1197: [695], 1200: [119], 1202: [82], 1206: [350], 1207: [29], 1208: [530], 1209: [78], 1211: [11], 1213: [243, 283, 601, 613, 653], 1215: [96, 146, 242], 1219: [20], 1220: [271], 1221: [58, 160, 806], 1222: [0], 1230: [6], 1232: [40], 1243: [26, 93], 1247: [233], 1251: [10], 1254: [29], 1261: [32], 1263: [358, 366, 671], 1265: [141], 1268: [133], 1269: [119, 569, 747, 967], 1272: [15, 41], 1275: [5, 42], 1276: [21], 1277: [1648], 1278: [51], 1282: [154, 190], 1288: [65], 1290: [10], 1306: [30], 1309: [40], 1314: [1], 1315: [249, 368], 1323: [2], 1328: [78], 1330: [60], 1339: [81], 1340: [23, 100, 178], 1345: [3], 1346: [87], 1351: [188, 401], 1352: [29], 1356: [113, 125, 139], 1358: [25, 68], 1365: [12], 1368: [21], 1370: [161, 179], 1374: [232], 1380: [40], 1383: [65, 213], 1384: [272], 1386: [23], 1394: [141, 152], 1403: [114], 1409: [178], 1412: [295, 384, 540, 585], 1413: [87], 1415: [55], 1418: [38], 1422: [353], 1434: [314, 499, 855], 1435: [2, 38], 1436: [114, 248], 1440: [310, 466, 620, 754, 849, 974, 1093], 1441: [72], 1445: [374], 1446: [15], 1455: [237], 1458: [63], 1460: [352], 1461: [124, 347, 354, 360], 1462: [158, 239, 264, 588], 1463: [249], 1469: [109], 1473: [42], 1477: [67], 1479: [66], 1482: [111], 1489: [93, 108], 1492: [353, 389], 1499: [358, 488], 1502: [7, 306, 682, 1311, 1425], 1504: [44, 838, 1345], 1513: [36], 1516: [60], 1522: [1400], 1524: [87], 1528: [502, 515, 661, 692], 1530: [58], 1535: [214, 226], 1536: [25], 1539: [15, 68, 159, 364, 626], 1541: [162], 1550: [405], 1552: [15, 235], 1556: [38, 60], 1559: [153, 356], 1560: [241, 335], 1561: [0, 506], 1565: [100], 1566: [57, 157], 1569: [26], 1573: [0], 1575: [38], 1576: [120], 1577: [169], 1578: [29, 122], 1581: [196], 1585: [4, 66], 1586: [313], 1596: [67], 1597: [80], 1601: [177], 1606: [47, 161], 1607: [20, 316, 405], 1608: [57], 1610: [23, 36], 1614: [130, 282], 1618: [292, 507, 610], 1620: [205], 1634: [123, 269, 400, 478, 718, 1502], 1638: [194], 1642: [0], 1644: [26], 1645: [203], 1647: [190, 274], 1648: [3, 8, 299], 1653: [65], 1656: [229, 709, 777], 1658: [1], 1661: [16], 1662: [204], 1663: [336, 572, 751], 1664: [386], 1667: [108, 381], 1669: [138], 1670: [145, 261], 1672: [50, 249, 301, 424], 1676: [58], 1682: [62], 1687: [170], 1688: [275, 709], 1689: [29], 1692: [95], 1693: [2], 1694: [398], 1696: [202], 1699: [64, 139], 1700: [166, 505, 1344, 1928], 1702: [111, 220, 227, 499, 527, 547], 1703: [47, 159], 1705: [771, 1294], 1707: [17], 1708: [4], 1711: [57, 62, 228, 276], 1712: [109, 115], 1715: [207, 216], 1723: [14, 103, 122], 1726: [9, 118], 1727: [185], 1728: [101, 255, 268], 1730: [67, 90, 119, 422], 1732: [211], 1734: [193, 206], 1741: [145, 368, 459], 1745: [4, 41], 1746: [8], 1747: [137], 1750: [6], 1752: [27], 1759: [400, 517], 1760: [138, 185], 1769: [74, 313, 881], 1771: [61, 545, 1168, 1179], 1774: [1], 1777: [54, 90, 181], 1784: [55], 1787: [12, 87, 103], 1793: [24], 1795: [31], 1796: [131], 1798: [342], 1802: [0, 51], 1804: [109], 1805: [9, 14, 93], 1807: [84], 1811: [71, 299], 1812: [6, 61], 1813: [513], 1815: [9], 1816: [430], 1817: [182, 1008], 1818: [613], 1820: [17], 1821: [51, 70], 1822: [81], 1824: [18], 1829: [71], 1832: [10, 157], 1835: [70], 1838: [27, 81], 1840: [72, 85, 220], 1842: [2, 9, 73], 1845: [55], 1848: [18], 1853: [289, 410, 469], 1857: [294], 1860: [11, 80], 1862: [181], 1864: [118], 1869: [68], 1870: [235, 376], 1871: [462, 885, 1080], 1874: [11, 62, 237], 1878: [48], 1879: [19], 1881: [290], 1882: [226], 1885: [2], 1887: [39, 120], 1890: [84, 1017], 1892: [298], 1893: [182], 1894: [197], 1898: [375], 1899: [41], 1902: [249, 292], 1904: [148], 1905: [197], 1908: [28, 102], 1912: [20], 1913: [89], 1917: [6], 1919: [123], 1920: [60, 96], 1921: [110], 1922: [78], 1925: [3, 118], 1926: [316, 391], 1927: [240], 1928: [8], 1937: [8, 28], 1940: [50], 1942: [104, 118], 1945: [56, 141], 1949: [156, 353, 493], 1950: [722, 732], 1952: [12, 184], 1953: [226, 594, 606], 1956: [92], 1957: [81], 1958: [173, 287, 435], 1960: [11], 1966: [937, 1041], 1967: [16], 1968: [112, 204], 1971: [34, 299], 1972: [120], 1976: [42], 1977: [72], 1978: [2], 1980: [74], 1982: [133], 1983: [235], 1984: [98, 168, 177], 1986: [3, 13, 179], 1990: [52], 1993: [35], 1995: [3, 155], 1997: [193], 1999: [1, 243], 2000: [363, 369, 415, 419, 495], 2001: [79, 301], 2002: [28], 2003: [192, 205, 581], 2004: [86], 2006: [45, 195, 762, 784], 2011: [250], 2013: [0], 2023: [63], 2026: [57, 160], 2030: [178], 2031: [7], 2033: [408], 2037: [112], 2038: [0, 6, 224], 2052: [7, 139], 2054: [296, 307, 1125, 1864, 2851], 2058: [312, 581, 604, 611, 758], 2060: [65], 2070: [2, 195], 2071: [62], 2072: [175], 2074: [102], 2075: [15, 195], 2076: [9], 2081: [43], 2085: [31], 2086: [528], 2088: [189], 2089: [167, 410], 2092: [41, 55, 200], 2093: [192, 375], 2094: [326, 336], 2097: [7], 2099: [29], 2104: [18, 166, 185], 2110: [3], 2114: [192, 402, 540, 556, 559, 586, 758, 780, 923], 2117: [277, 280], 2118: [198], 2125: [135, 256, 555], 2129: [829, 875], 2133: [40, 116, 128, 131, 372, 438, 443], 2134: [31], 2144: [201], 2147: [204, 583], 2148: [117, 393, 487, 783], 2149: [27, 191], 2150: [47, 224], 2152: [202], 2155: [4, 39, 192], 2159: [30], 2162: [550, 829, 1020, 1366, 1507], 2167: [78], 2168: [269, 365], 2169: [77, 180, 445], 2170: [47], 2178: [87], 2179: [96], 2180: [482], 2181: [38], 2182: [14, 204, 230, 292], 2186: [42, 44, 99], 2192: [34], 2201: [499], 2206: [49, 130, 203], 2207: [91, 267, 327], 2212: [71], 2214: [40], 2216: [9, 175], 2219: [163], 2220: [3, 343, 710], 2221: [317], 2224: [3, 90, 388], 2228: [60], 2233: [236], 2234: [81, 99, 135], 2242: [131], 2247: [35, 230, 251, 257, 373], 2250: [90], 2251: [32], 2252: [62], 2258: [32], 2259: [272], 2261: [45, 589], 2265: [20], 2271: [45], 2272: [370], 2276: [600], 2280: [473], 2281: [21, 53], 2282: [0, 98], 2293: [66, 106], 2296: [38], 2298: [57], 2303: [361], 2305: [349, 353], 2313: [10, 167, 190, 622, 639], 2318: [153, 242], 2321: [48], 2323: [61], 2333: [318, 337], 2336: [14], 2337: [64], 2338: [39], 2339: [401], 2342: [80], 2346: [5, 285], 2349: [79], 2350: [845], 2351: [87, 179, 307], 2352: [115, 414], 2357: [163], 2359: [166, 366], 2364: [91, 278, 370, 417, 422, 994, 1125], 2370: [80], 2371: [47], 2374: [1, 79, 91, 117, 302], 2376: [46, 103, 159], 2382: [9], 2383: [48], 2386: [74, 265, 297], 2388: [38, 119], 2391: [88], 2392: [69, 436, 554], 2396: [563], 2398: [182, 187], 2399: [78], 2405: [303, 445, 492, 561, 652, 1012], 2408: [66, 249, 574], 2422: [179], 2424: [28, 650], 2432: [90], 2436: [168], 2439: [31, 108, 137], 2440: [362, 366], 2443: [138, 166], 2447: [49, 53], 2448: [243, 844, 918], 2449: [5, 310], 2459: [101, 208, 229], 2461: [105, 428], 2462: [16], 2470: [364], 2471: [52, 79], 2476: [653], 2477: [42, 279], 2498: [3, 234], 2500: [152], 2502: [2], 2505: [0], 2511: [133], 2517: [646, 876], 2519: [452, 455, 584, 660, 668], 2522: [57, 124], 2525: [253, 496, 598], 2531: [22], 2537: [30], 2541: [49], 2548: [0, 133], 2551: [2, 26, 383, 388], 2552: [1], 2559: [41], 2561: [48], 2564: [1, 23], 2567: [0], 2570: [6], 2572: [16, 273, 334, 487], 2575: [195], 2577: [130], 2578: [76, 80], 2579: [57], 2580: [179], 2582: [224], 2587: [223], 2588: [730], 2590: [179], 2592: [58, 73, 92, 106], 2601: [98], 2606: [82], 2609: [543], 2612: [49], 2615: [4], 2617: [17], 2625: [88], 2627: [77], 2629: [5, 37], 2635: [11, 129], 2636: [11, 109, 126, 134], 2639: [52], 2642: [49, 68], 2644: [313], 2647: [65], 2650: [38], 2656: [262], 2657: [285], 2659: [73], 2661: [262], 2663: [9, 28], 2674: [5, 51], 2677: [2], 2679: [380], 2686: [15, 195], 2689: [116], 2691: [4], 2692: [154, 986], 2694: [253], 2697: [10, 23, 386, 451], 2699: [172], 2710: [100, 399, 410, 423, 446, 660, 727], 2714: [382], 2716: [136, 146], 2717: [55], 2718: [282, 430], 2720: [67, 376], 2723: [3], 2733: [4], 2734: [290], 2737: [126], 2739: [421], 2740: [154], 2741: [87], 2742: [27], 2744: [160], 2747: [14], 2749: [182], 2751: [15], 2752: [149, 244, 347, 376, 552, 914, 942], 2753: [43, 110], 2756: [31], 2757: [174], 2758: [85], 2759: [994, 1130], 2760: [190], 2761: [226, 239], 2762: [201, 231], 2764: [17], 2767: [17], 2772: [453], 2773: [253], 2775: [135, 137], 2778: [34], 2784: [26], 2785: [249], 2792: [128], 2793: [46], 2794: [77], 2795: [237, 427], 2796: [217, 245, 882], 2797: [94], 2810: [82, 940], 2814: [4], 2817: [46], 2826: [151, 328], 2827: [10], 2829: [112, 427], 2830: [33, 86], 2843: [78], 2845: [16], 2849: [575], 2850: [74], 2854: [280], 2855: [32], 2860: [463, 470], 2863: [110, 427, 556], 2864: [32], 2873: [190], 2875: [9], 2880: [10], 2881: [120], 2884: [3, 223, 264], 2885: [102, 245, 263], 2886: [93], 2893: [382, 654], 2897: [171], 2899: [27], 2902: [632], 2908: [386, 481], 2910: [34], 2911: [42, 193], 2921: [32], 2923: [428, 460, 517], 2925: [39], 2929: [198, 692], 2930: [145], 2934: [871, 891, 921], 2938: [98, 147], 2940: [297], 2943: [13, 168], 2945: [77], 2950: [0], 2952: [434, 496], 2955: [85, 316], 2957: [107, 125], 2963: [218, 232, 323], 2964: [696, 705, 1035, 1048], 2966: [33, 41], 2968: [86], 2969: [59], 2975: [15, 231], 2977: [71], 2979: [19, 112], 2982: [21], 2986: [81], 2989: [97], 2999: [0, 12, 131, 480, 502, 623]}\n",
      "\n",
      "trustee\n",
      " {0: [3], 444: [61, 300], 537: [20], 556: [21], 1033: [644], 1215: [238], 1306: [107], 1862: [208], 1874: [162], 2785: [739]}\n",
      "\n",
      "among\n",
      " {0: [4], 4: [366], 6: [89], 7: [77], 16: [377], 30: [542, 856], 46: [467], 50: [73], 60: [55], 67: [165], 70: [7], 71: [74], 75: [262], 87: [272, 361], 106: [159], 133: [54], 138: [85], 141: [354], 148: [51, 86], 157: [28, 57, 76], 158: [249], 163: [368], 171: [87], 180: [17], 183: [234], 188: [166, 177], 194: [14], 222: [291], 250: [218], 278: [250], 279: [1220], 310: [100], 311: [272], 318: [236], 333: [87], 335: [166], 349: [80], 361: [404, 424], 381: [298], 387: [62], 395: [77], 406: [48, 161], 424: [94], 425: [131], 447: [29], 448: [147], 451: [502], 473: [66], 486: [91], 491: [101], 498: [793], 503: [81], 505: [24], 521: [274, 1226], 538: [95], 547: [59], 577: [469], 579: [272], 590: [319], 593: [10], 608: [161], 610: [177, 184], 617: [49], 636: [260], 658: [77], 659: [15], 662: [242], 666: [272], 672: [315], 674: [177], 696: [227], 707: [6], 712: [127], 720: [376], 725: [133], 730: [1185], 734: [381], 777: [300], 782: [182], 798: [107], 812: [770], 821: [50, 74], 845: [28], 847: [94], 851: [1297], 852: [65], 861: [93], 873: [765], 879: [31], 882: [79, 91], 883: [239], 887: [67], 889: [236], 893: [107], 896: [119], 899: [27], 908: [25], 917: [110], 933: [64, 223], 934: [11], 950: [364], 951: [105], 954: [104], 955: [159], 962: [73], 966: [71], 985: [75], 988: [16], 990: [309], 999: [46], 1045: [105], 1090: [167], 1092: [53, 461], 1102: [51], 1112: [101], 1115: [377], 1123: [139], 1140: [196], 1185: [1141], 1205: [56], 1226: [61], 1263: [675], 1269: [183, 731, 785], 1306: [101], 1356: [127, 166], 1396: [40], 1440: [847], 1454: [21], 1499: [119, 492], 1502: [1016], 1504: [830], 1520: [259], 1522: [337], 1525: [85], 1539: [23], 1549: [213], 1591: [747, 816], 1600: [343], 1613: [65, 115], 1632: [152], 1634: [1381], 1640: [251, 383], 1651: [44], 1660: [132, 549], 1667: [38, 407], 1669: [94, 177], 1671: [773], 1672: [407, 439, 705], 1673: [409], 1685: [165], 1687: [122], 1705: [770], 1712: [111], 1715: [389, 396], 1724: [838, 1395], 1727: [224], 1737: [6], 1754: [59, 63, 70], 1760: [34, 538], 1761: [420], 1784: [60], 1793: [294], 1794: [73], 1805: [51], 1809: [257], 1824: [77, 239], 1863: [303], 1866: [134], 1870: [26], 1876: [404], 1887: [230], 1894: [396], 1897: [178], 1913: [936], 1924: [46], 1926: [175], 1939: [110], 1945: [171], 1947: [86], 1953: [135], 1958: [85], 1962: [65], 1966: [1011], 1972: [68], 1979: [57], 1983: [94], 1994: [23], 1997: [148, 254], 2016: [38], 2055: [191], 2065: [534], 2074: [84], 2086: [452], 2103: [192], 2111: [243], 2120: [52], 2129: [539], 2160: [99], 2168: [329, 339], 2179: [141, 448], 2188: [27], 2225: [42], 2233: [112, 463], 2235: [354], 2242: [86], 2276: [0], 2284: [36], 2292: [81], 2296: [219], 2313: [691], 2337: [246], 2339: [347], 2347: [47], 2350: [245], 2361: [213], 2364: [239], 2392: [129], 2405: [952], 2423: [185], 2434: [124], 2443: [70], 2445: [81], 2446: [23], 2448: [580], 2449: [607], 2451: [86], 2461: [36], 2475: [55], 2492: [271], 2519: [85], 2553: [133], 2569: [71], 2583: [6], 2587: [105], 2588: [155], 2594: [29], 2602: [58], 2647: [296], 2688: [21], 2692: [290], 2694: [207], 2703: [278], 2720: [176], 2759: [124], 2761: [11], 2788: [439], 2839: [55], 2862: [54], 2863: [28, 132], 2865: [18], 2887: [44], 2899: [106], 2902: [742], 2906: [253], 2913: [10], 2940: [543], 2952: [85], 2954: [165], 2987: [392]}\n",
      "\n",
      "morrow\n",
      " {0: [5], 773: [452], 1863: [354]}\n",
      "\n",
      "day\n",
      " {0: [6, 215], 4: [164], 16: [303, 842], 18: [72], 24: [525], 26: [68], 27: [4], 31: [335], 38: [60, 320, 405, 412], 40: [24], 41: [358], 42: [23], 45: [130, 138, 353], 46: [129, 130], 49: [225], 56: [549], 60: [177], 62: [393], 65: [185], 67: [62], 69: [786], 73: [111, 214], 75: [238], 76: [132, 166, 428], 79: [45, 47], 80: [7], 81: [11], 82: [204, 234, 304], 83: [81, 185], 87: [249, 347, 505], 98: [37], 107: [221, 374], 109: [105, 385], 112: [60, 73, 134, 352], 113: [48, 77], 114: [158], 117: [62], 124: [206], 128: [60], 139: [327, 418], 140: [141], 143: [35], 149: [0], 151: [453, 471], 159: [32, 282, 283], 162: [316], 163: [127, 236, 921], 164: [335, 471, 798], 168: [36], 170: [69, 126], 172: [563], 174: [40], 178: [15, 21], 181: [94], 183: [788], 184: [440, 676], 186: [20], 187: [13], 188: [189], 189: [40, 135], 190: [26], 191: [560, 765], 193: [542, 544, 646, 1043, 1045, 1197, 1232, 1332, 1377], 198: [19], 202: [301], 213: [210], 214: [75, 77, 428], 220: [49], 222: [21, 342, 344], 230: [242], 236: [14, 274, 303], 240: [152, 206], 245: [414], 250: [89, 135, 242, 516], 253: [154], 254: [10], 255: [96, 164], 259: [244], 260: [226, 232, 258], 267: [87], 269: [142], 280: [263, 268, 364], 282: [66], 288: [45], 289: [127, 436], 293: [40, 141, 195, 387, 429, 461], 294: [97], 295: [4, 53], 296: [437], 299: [11], 310: [262, 566], 311: [621], 312: [687], 317: [21], 324: [5, 38, 215], 327: [418], 328: [18], 329: [42], 331: [371], 334: [77, 145, 328, 413], 337: [54, 68], 339: [32], 341: [230, 839, 927], 342: [296], 343: [15], 345: [513, 790, 995, 1020], 349: [38, 41, 68], 354: [58, 62], 365: [1], 368: [190, 306, 413, 514], 372: [24, 103], 374: [383, 400], 375: [300], 379: [47], 381: [234, 416], 383: [267], 386: [31], 391: [102], 397: [217, 230], 401: [172], 407: [48], 408: [358], 409: [59], 410: [1125], 412: [538], 414: [370], 417: [91, 141], 423: [352, 377], 424: [47, 57, 67], 425: [95], 429: [82, 86], 431: [136], 439: [312], 440: [12], 442: [107, 169, 213, 269, 395, 460], 451: [588], 453: [203, 264], 462: [91, 365, 371], 467: [115], 473: [156, 243, 251, 381, 404, 417], 477: [1, 183], 481: [923, 1092, 1126, 1389], 486: [75, 284, 544], 493: [108], 497: [333], 498: [673, 715], 505: [249, 442], 506: [190, 203], 509: [43], 512: [27], 513: [36], 521: [467, 721, 829, 834, 962, 966, 968, 1142, 1158], 523: [86], 536: [10, 103], 540: [16], 544: [34], 546: [75], 549: [133], 553: [173, 226], 557: [45, 150], 560: [59], 561: [3, 92, 104, 141], 570: [260], 577: [8, 107], 585: [24, 34], 587: [247, 291, 352, 454, 495, 531], 598: [7, 134], 602: [237], 610: [172, 173], 611: [172], 613: [187, 508], 618: [71, 86, 315], 619: [56], 627: [346], 632: [4, 27], 641: [87], 646: [59], 651: [258, 693], 657: [58], 666: [87, 89, 113, 156], 672: [196], 677: [25], 679: [128], 680: [36, 61], 684: [138, 1046, 1054], 688: [42, 75], 689: [14], 695: [27], 697: [100], 714: [611], 716: [12], 717: [10], 726: [32], 728: [36, 104], 743: [630], 753: [27], 759: [144, 190, 232, 268, 340], 773: [381], 777: [172, 236, 242], 781: [61, 392, 517], 785: [4], 790: [23, 53], 800: [6, 96, 231, 263, 344, 409, 460], 818: [54, 220, 331, 341], 821: [141, 143], 825: [36, 566, 745], 827: [64], 829: [44, 243], 833: [20, 211], 844: [248], 847: [129], 852: [204, 277, 447], 861: [520], 871: [80], 874: [115], 876: [54, 70], 880: [43], 881: [202], 885: [19], 888: [278], 889: [188], 894: [2, 135, 339], 897: [9, 201], 900: [1], 904: [83, 94], 914: [300], 917: [65, 138, 388], 919: [186], 924: [44], 925: [625], 927: [44], 928: [144], 932: [225], 934: [0], 947: [164], 955: [94, 305, 467], 961: [162, 376], 971: [48, 153], 975: [164], 981: [47], 982: [166, 228, 246, 261, 406, 891], 985: [51], 986: [161], 990: [90], 991: [26], 1001: [111, 130, 160, 173], 1002: [113], 1008: [18], 1012: [16], 1016: [78], 1019: [113, 130], 1020: [18, 342], 1022: [60], 1026: [187], 1027: [102], 1033: [151, 358, 446], 1040: [4, 673, 1060], 1041: [142, 406], 1051: [290], 1053: [257, 416, 608], 1057: [9, 770], 1062: [92, 177], 1065: [27], 1073: [12], 1074: [92], 1081: [168, 442], 1088: [107, 162], 1089: [170, 187, 211], 1090: [76, 220, 314, 497], 1091: [137, 148], 1092: [258, 304], 1097: [429], 1109: [134, 208], 1112: [58, 167], 1120: [377], 1128: [25], 1131: [226], 1132: [203, 489], 1136: [92, 172, 506], 1137: [2], 1140: [461, 549], 1141: [21], 1142: [199, 272], 1148: [7], 1149: [223], 1150: [93, 572], 1154: [120], 1156: [65, 239, 302, 426], 1162: [358], 1163: [28, 139, 193, 201], 1164: [290, 596], 1168: [117], 1174: [323], 1175: [137], 1176: [204], 1177: [280, 352, 457, 542], 1180: [252], 1185: [129, 174, 265, 415, 554, 995, 1014], 1188: [203], 1189: [699, 865], 1191: [14], 1194: [115, 146, 159], 1195: [11], 1197: [98, 146, 358, 365, 391, 419, 682], 1198: [54, 204, 298, 349], 1201: [61], 1203: [41, 47, 121, 265], 1208: [321], 1213: [123, 480], 1215: [120], 1221: [613], 1223: [53], 1225: [24], 1233: [53, 345], 1234: [32], 1235: [63], 1243: [144, 392], 1249: [67], 1253: [50], 1258: [4], 1263: [390, 760], 1269: [152, 293, 326, 493, 585, 640, 728, 748, 755, 845, 903, 922], 1271: [210, 295], 1274: [42], 1275: [37, 53, 225, 239, 615, 618], 1277: [1354, 1483, 1489, 1605, 1738, 2223, 2348], 1282: [217, 429], 1285: [43], 1294: [113], 1299: [137], 1300: [76], 1306: [232, 292, 296], 1311: [113], 1316: [131], 1317: [43], 1322: [108], 1323: [26, 136, 421], 1327: [24, 31, 37, 70], 1349: [185, 284, 306], 1351: [49, 51, 431], 1356: [59, 183], 1358: [17], 1360: [104, 114], 1366: [5], 1369: [4], 1370: [150], 1372: [101, 235], 1373: [59], 1374: [90], 1377: [3], 1380: [21], 1381: [52], 1383: [313], 1385: [21], 1392: [71], 1394: [46], 1399: [30], 1402: [37], 1403: [526], 1409: [122], 1411: [470], 1412: [696, 720], 1419: [67, 560], 1420: [7], 1422: [102, 143, 393, 427], 1430: [596, 643], 1431: [184], 1434: [126, 401, 696, 699, 714], 1436: [80], 1438: [168], 1439: [9], 1440: [451, 725, 1054], 1443: [55], 1445: [110], 1450: [82], 1460: [96, 388], 1462: [255, 478, 576], 1469: [122, 190], 1475: [70, 123, 219, 228], 1482: [174, 337], 1484: [77], 1489: [13], 1492: [502], 1499: [67, 353], 1502: [1011], 1503: [61, 129, 302], 1504: [143, 149, 378, 720], 1505: [160, 321], 1507: [81], 1511: [224], 1512: [32], 1515: [148, 180], 1520: [425], 1522: [236, 354, 1102], 1524: [120, 293], 1532: [296], 1536: [67], 1540: [4, 35], 1541: [221, 273], 1543: [19], 1546: [25, 46, 56, 63, 194], 1549: [322], 1550: [110], 1551: [102], 1552: [267], 1553: [26], 1557: [334], 1559: [405], 1560: [99], 1561: [92, 212, 261], 1564: [302], 1569: [143, 159, 216], 1575: [63, 65], 1577: [226], 1578: [72, 167, 246], 1585: [95], 1586: [276, 289], 1591: [709, 830], 1599: [70], 1600: [151], 1601: [73, 146], 1606: [429], 1615: [59], 1618: [89], 1620: [134, 288], 1624: [313], 1634: [183, 318, 536, 1345, 1350, 1730, 2019, 2062, 2104, 2256, 2269, 2320, 2441], 1636: [118], 1643: [138, 359], 1645: [58, 80, 104, 149], 1648: [74, 244], 1650: [121], 1656: [171, 207, 312, 611, 637, 838, 867], 1659: [220], 1661: [243], 1662: [162], 1663: [332, 388], 1664: [272, 342], 1667: [182], 1669: [233], 1671: [712, 791], 1672: [101], 1673: [203], 1683: [127], 1685: [229, 235], 1687: [19], 1688: [85, 249, 250, 342, 589, 685], 1689: [286, 327, 506], 1691: [5], 1692: [51], 1696: [203, 257, 268, 325], 1699: [28, 66], 1700: [26, 1286, 2478], 1709: [322, 360], 1716: [38], 1719: [434, 662], 1720: [164], 1724: [590, 1201], 1726: [64], 1729: [81], 1733: [5], 1734: [262], 1735: [26], 1737: [14, 120], 1741: [34, 316, 435], 1745: [109], 1749: [29], 1750: [18], 1755: [157, 175], 1758: [73], 1759: [755], 1760: [564, 618, 757], 1761: [88, 357], 1762: [203], 1763: [924], 1769: [148, 459, 519], 1771: [496, 707, 769, 1080, 1142], 1779: [5, 114, 259], 1780: [7], 1788: [219, 430], 1794: [245, 246], 1798: [30, 156], 1802: [153], 1804: [26], 1806: [201], 1809: [126], 1811: [72, 311], 1817: [168, 197, 296, 424, 748, 763, 936, 1186, 1192], 1818: [553, 588, 952, 976, 994], 1819: [32, 40, 41], 1822: [53], 1824: [109], 1829: [195], 1830: [305], 1832: [23], 1833: [79], 1834: [11], 1839: [8], 1841: [54], 1844: [25, 253, 289], 1845: [246], 1851: [15, 45], 1853: [635], 1854: [219], 1855: [219, 283, 376], 1857: [163, 363], 1862: [39, 212, 429], 1863: [267, 424], 1864: [185], 1867: [123, 130], 1868: [54, 189], 1871: [296, 308, 323, 446, 647, 688, 893, 1296, 1894], 1890: [243, 260, 444, 596, 686, 694, 812, 954, 961, 1541, 1547], 1892: [167], 1897: [322], 1898: [147], 1902: [189, 333, 372], 1904: [425], 1905: [131, 133, 137, 228], 1908: [92, 268], 1916: [85, 255], 1919: [256], 1920: [457], 1923: [205, 351], 1932: [38], 1945: [73, 259], 1947: [123], 1949: [205, 206], 1950: [239], 1952: [13], 1953: [227, 354, 573], 1954: [27], 1956: [397, 467], 1958: [42, 174], 1959: [328], 1964: [167, 185], 1970: [366], 1971: [69, 159], 1977: [313], 1983: [144, 596], 1984: [2], 1990: [229, 254], 1993: [26], 1994: [414], 1995: [1, 174], 1997: [157, 160, 176, 194, 211, 268], 1999: [16, 229, 230], 2001: [296], 2002: [177], 2003: [55, 64, 790, 833], 2006: [40, 41, 61, 96, 101, 216, 252, 326, 412, 439, 622, 720], 2012: [161, 286, 379], 2016: [65, 122, 136, 204, 299, 327], 2019: [24], 2020: [2, 80], 2021: [16, 33], 2022: [46], 2023: [113], 2026: [3], 2029: [11], 2033: [329, 349], 2034: [46, 75, 387], 2038: [67], 2039: [24], 2043: [11, 38], 2044: [43], 2052: [64, 92, 105, 261, 265, 282, 415, 457], 2054: [75, 615, 661, 671, 1361, 1377, 1393, 1558, 1915, 1932, 2416, 2519, 2681, 3000, 3065], 2058: [343, 772], 2059: [9], 2065: [186], 2066: [63, 280], 2069: [58, 99, 262], 2074: [103], 2075: [26], 2079: [88], 2080: [96, 522], 2082: [22], 2087: [143], 2089: [224, 309], 2092: [90, 112], 2094: [167, 357], 2095: [66, 67], 2097: [24], 2101: [0, 3], 2102: [159], 2109: [186], 2111: [364], 2114: [21, 70, 220, 259, 294, 319, 339, 348, 394, 509, 606, 615, 706, 752], 2117: [222], 2120: [127, 342], 2121: [63, 64], 2122: [27], 2127: [51, 247], 2133: [191], 2134: [12], 2136: [86, 114], 2139: [198], 2147: [210, 219], 2148: [206, 245, 567], 2150: [93, 102], 2152: [309], 2154: [82, 246, 305], 2158: [23], 2159: [128], 2160: [42], 2161: [3], 2162: [677, 1191, 1467, 1473], 2164: [7], 2167: [275], 2172: [160, 177, 310, 428], 2174: [235], 2179: [204, 430, 469, 739], 2180: [194], 2182: [42], 2184: [64], 2186: [45], 2187: [59], 2188: [266], 2189: [176], 2190: [118], 2196: [551, 664], 2197: [237, 252, 302, 373], 2201: [1041], 2204: [45, 70], 2206: [80, 84], 2207: [138], 2208: [482, 623, 653, 794], 2210: [48], 2211: [28], 2212: [21], 2216: [159], 2236: [389, 411, 493], 2237: [24], 2238: [64, 74], 2242: [392], 2244: [12, 31, 206], 2245: [108], 2247: [106, 194, 309, 332], 2250: [461], 2251: [5], 2252: [61, 168], 2253: [53], 2255: [128], 2259: [11, 255], 2261: [515, 527], 2266: [48], 2267: [76], 2276: [72, 141, 945], 2280: [69, 139, 221], 2281: [1], 2284: [46, 108], 2285: [28, 34, 187, 242], 2286: [143], 2287: [105, 534, 688], 2290: [28], 2296: [39], 2301: [205], 2313: [388], 2321: [49, 56], 2322: [121], 2323: [1], 2330: [119], 2333: [141, 177, 279, 315, 354, 364, 428], 2334: [26], 2336: [15], 2338: [4], 2339: [334], 2346: [3, 184, 202], 2350: [474, 502, 684, 846], 2352: [630], 2354: [871], 2355: [230, 346], 2359: [200], 2364: [421, 449, 505, 854], 2365: [72], 2367: [160], 2368: [50], 2369: [79], 2371: [115, 143], 2372: [152], 2373: [124], 2380: [22, 478, 501], 2386: [298], 2388: [46, 60], 2390: [45], 2392: [222], 2393: [68, 111], 2395: [118], 2396: [8], 2405: [217, 218, 530, 545, 897], 2406: [76, 176], 2407: [402], 2418: [4, 296], 2419: [69], 2424: [172, 179, 295], 2432: [26], 2434: [128], 2436: [159], 2437: [6], 2440: [494], 2441: [369], 2448: [5, 804, 1002], 2449: [12], 2451: [412], 2456: [128], 2465: [127], 2476: [175, 536], 2487: [248], 2490: [173], 2496: [56], 2498: [681], 2500: [307, 619, 691, 698, 711, 712, 745, 870, 893, 954, 1050, 1110, 1155, 1257, 1422, 1905], 2506: [81], 2507: [50], 2509: [8, 183], 2515: [99], 2517: [160, 505, 512, 584], 2518: [426], 2519: [217], 2525: [66, 75, 228, 371, 403, 411], 2534: [401], 2536: [94], 2540: [37, 415], 2543: [63], 2549: [11], 2550: [26], 2551: [42, 177, 206, 283], 2553: [83], 2555: [301, 318, 331, 558], 2556: [24], 2558: [64, 65], 2561: [141], 2564: [24, 31], 2568: [70], 2572: [134, 418], 2575: [231, 315, 381], 2583: [282], 2586: [6], 2587: [296], 2588: [698, 740, 931], 2599: [50], 2601: [53], 2610: [74, 225], 2613: [63, 72, 76, 227], 2615: [17], 2625: [16, 102], 2631: [57], 2643: [52], 2644: [289], 2647: [355, 399], 2649: [98], 2656: [121], 2657: [16], 2666: [108], 2668: [338], 2670: [116], 2675: [26, 93], 2676: [225, 227], 2679: [60, 76, 129, 153, 199, 299, 349, 520, 599, 1055], 2680: [17], 2692: [368], 2697: [393], 2701: [257], 2702: [135], 2705: [63], 2707: [119, 122, 136, 151], 2710: [105, 412, 415, 630, 825], 2712: [14], 2714: [3, 359, 363], 2715: [24], 2718: [60, 439], 2719: [24], 2720: [76], 2727: [74], 2734: [233], 2739: [217], 2743: [8], 2745: [143], 2752: [234, 743, 746, 755, 799, 807, 831], 2755: [44], 2756: [18], 2759: [922, 969], 2761: [154, 173, 189], 2764: [408], 2766: [85], 2768: [65], 2770: [187], 2772: [201], 2774: [111], 2784: [94], 2785: [669], 2787: [110], 2788: [503], 2795: [104, 295, 447, 485, 551], 2796: [9, 391], 2806: [27, 32], 2810: [614, 805], 2814: [45], 2817: [62], 2825: [15], 2826: [343, 352], 2833: [3], 2836: [30], 2838: [123], 2841: [112], 2847: [59], 2852: [213], 2853: [30, 41], 2854: [38], 2858: [160, 279], 2860: [507, 516], 2862: [64], 2866: [383], 2873: [21], 2879: [138, 220, 677, 1014], 2881: [66, 186], 2882: [100], 2893: [783], 2895: [29], 2898: [23], 2902: [196, 267, 520, 634, 646], 2904: [289, 492], 2905: [8], 2906: [426], 2907: [1, 28, 56], 2908: [203], 2909: [566], 2910: [32], 2916: [104], 2923: [15, 45], 2929: [164, 712], 2934: [889, 937], 2938: [49, 120, 148], 2940: [92, 246, 273, 459], 2942: [33], 2943: [267, 732], 2951: [2], 2952: [471, 583], 2954: [204], 2956: [63, 260], 2963: [338, 386, 406, 480], 2964: [114, 298, 496, 662, 941], 2965: [15, 41, 100], 2973: [172], 2981: [299], 2982: [10, 34], 2989: [16, 143], 2991: [303]}\n",
      "\n",
      "arthur\n",
      " {0: [7, 20, 26, 58, 70, 83, 90, 106, 108, 150, 161, 171, 175, 180, 196, 208, 220, 232, 236, 243, 244, 261, 288, 319, 355, 380, 404, 422], 18: [74], 42: [57, 185, 223, 224, 233, 247, 257], 44: [51, 55, 68, 81, 105, 134, 140], 108: [9, 53, 88, 114, 143, 203, 219, 239, 247, 257, 305, 320, 327, 332, 340, 346, 389, 400, 431, 448, 462, 468, 472], 143: [305], 162: [330, 350, 352, 366, 375, 377, 383, 386, 418, 427], 182: [103, 129], 193: [12, 97, 102, 302, 309, 357, 361, 365, 485, 504, 539, 581, 599, 705, 741, 769, 783, 803, 820, 852, 859, 953, 1047, 1052, 1431, 1448, 1468, 1478, 1635], 293: [118, 135, 165, 192, 209, 270, 293, 350, 359, 378, 388, 405, 455, 571], 341: [31, 180, 187, 292, 296, 712, 744, 753, 866, 1000, 1008], 362: [129], 399: [9, 19, 38, 40, 107], 412: [2, 11, 1003, 1012], 436: [113], 444: [2, 21, 80, 106, 119, 127, 167, 189, 195, 212, 230, 239, 257, 279], 446: [49, 65, 120, 134, 140, 147, 350, 352], 500: [1, 14, 144, 161], 543: [4, 16, 24, 98, 111, 198, 209, 215, 223], 604: [3], 613: [12], 686: [0, 49, 56, 68, 80, 98, 146, 214, 270, 276, 301], 709: [8, 27, 64, 180, 319, 343, 401, 418, 425], 857: [43], 864: [257], 875: [78, 106], 889: [260], 939: [230, 241, 272, 286, 298, 312, 324, 368, 426, 444, 447, 458, 463, 469, 498, 522, 529, 563, 577, 584, 616, 670, 672, 718, 752, 778, 862, 874], 972: [37], 1097: [23], 1182: [49], 1187: [15], 1363: [49], 1417: [4], 1460: [17], 1507: [8, 26, 53, 83, 103, 163], 1514: [33], 1521: [54], 1530: [245], 1536: [8, 33, 108, 114, 121, 133, 149, 154, 167, 177, 188, 192, 195, 200, 213, 218], 1537: [3, 15, 30, 62, 73, 78], 1554: [2, 84, 148, 169, 208], 1578: [314], 1582: [3, 39, 61], 1591: [1, 24, 77, 113, 368, 711, 775, 776, 836], 1647: [153], 1660: [15], 1673: [4, 16, 81, 115, 167, 244, 275, 296, 348, 401], 1681: [55, 110, 122, 124, 145, 199], 1700: [874], 1705: [10, 45, 118, 217, 221, 247, 356, 364, 370, 377, 435, 454, 468, 501, 518, 532, 546, 565, 569, 600, 865, 888, 899, 987, 1021, 1036, 1053, 1063, 1073, 1096, 1151, 1160, 1199, 1209, 1247, 1248, 1274, 1276, 1288, 1300, 1305, 1332, 1336, 1338, 1350, 1376, 1442, 1454, 1463, 1467, 1486], 1711: [177, 180, 291], 1730: [411], 1754: [5, 38, 77], 1757: [133], 1785: [2], 1821: [15, 37, 67], 1871: [280, 658, 980], 1892: [123, 169], 1901: [6, 28, 88, 116, 137, 144, 179, 181, 184, 200, 227, 229, 253, 258, 264], 1913: [622], 1925: [11, 49], 1940: [27], 1946: [8], 1955: [93], 1970: [79, 82, 86, 164, 227, 238, 263, 285, 489], 1972: [12, 23, 30, 88, 102, 118], 1985: [24, 37, 53, 67, 108, 117, 123], 2261: [297], 2282: [103], 2363: [196], 2376: [212], 2392: [229, 379], 2439: [214], 2443: [235], 2536: [29], 2679: [131], 2721: [44], 2792: [80, 85], 2939: [93]}\n",
      "\n",
      "side\n",
      " {0: [8], 7: [518], 20: [89], 31: [341], 33: [135], 34: [176], 52: [77], 54: [251], 57: [85], 69: [1042], 70: [445], 87: [499, 658, 659, 661], 101: [217], 115: [249], 118: [169], 123: [316], 141: [502], 151: [232], 165: [147], 168: [173], 171: [482], 183: [452, 791], 184: [747], 190: [449], 193: [995], 202: [215], 204: [238], 208: [407], 209: [113], 232: [444], 257: [15], 260: [368, 423], 261: [188], 279: [1424], 302: [339], 310: [597], 316: [372], 350: [211], 352: [811], 363: [484], 366: [83], 395: [332, 433], 400: [346], 404: [120], 405: [272], 417: [417], 427: [137], 429: [672, 682, 703], 430: [805, 854, 937, 938], 438: [281], 442: [418], 471: [149], 478: [273], 481: [983, 1006, 1013, 1205], 497: [420], 498: [601], 500: [122], 512: [422], 513: [430], 561: [40], 581: [73], 584: [219], 613: [589], 615: [156], 621: [535], 625: [127], 626: [272], 633: [58], 647: [36], 652: [467], 654: [210], 665: [11], 678: [51, 53], 680: [316], 684: [212, 227, 1106, 1107, 1113, 1170, 1213, 1510], 694: [41], 706: [176], 708: [652], 717: [77], 733: [460], 734: [419], 737: [62], 742: [49], 747: [128], 760: [104], 770: [192], 781: [494], 783: [7], 795: [11, 12], 812: [1194], 821: [58], 825: [412], 832: [220], 842: [214], 861: [565], 872: [11], 879: [714, 915], 881: [6], 897: [226], 900: [169, 240], 919: [39], 928: [744], 937: [79], 946: [129, 267], 949: [114], 951: [668], 952: [12], 955: [326], 986: [546], 992: [220, 229], 1001: [177, 433], 1043: [35], 1045: [260, 261], 1057: [558], 1074: [44], 1115: [450], 1117: [106], 1120: [392], 1140: [495], 1154: [98], 1175: [532], 1183: [133], 1194: [372], 1213: [333, 812], 1215: [504], 1219: [360], 1233: [175], 1237: [80], 1275: [148, 660, 797], 1277: [2], 1315: [62], 1317: [36], 1322: [160], 1369: [54], 1403: [353], 1431: [147], 1484: [72], 1502: [622], 1503: [238], 1505: [188], 1514: [39], 1522: [49], 1525: [32, 159], 1528: [569], 1533: [1], 1549: [852], 1561: [768], 1573: [259], 1575: [85], 1600: [135, 246], 1601: [123], 1606: [236], 1640: [279], 1644: [82], 1645: [245], 1647: [169], 1656: [440], 1660: [669], 1664: [293, 419], 1671: [853], 1689: [397], 1694: [313], 1695: [167], 1696: [287], 1700: [1624, 2053, 2180], 1705: [1440], 1709: [63, 167, 220], 1724: [836], 1730: [563], 1744: [149], 1771: [1086, 1087], 1790: [55], 1818: [454], 1824: [131], 1838: [243], 1844: [321], 1855: [174], 1857: [687, 864], 1863: [132], 1874: [154], 1876: [373], 1893: [421], 1913: [203], 1915: [4], 1918: [27, 33, 66, 90], 1932: [180], 1943: [73], 1959: [263], 1966: [743], 1970: [136], 1999: [57], 2000: [395], 2003: [768], 2012: [75, 247], 2032: [104], 2038: [65], 2062: [233], 2063: [116], 2065: [175], 2087: [43], 2088: [56], 2117: [43], 2128: [192], 2129: [126, 265], 2148: [647], 2150: [84, 356], 2152: [274], 2162: [475], 2177: [178], 2179: [663], 2218: [105], 2224: [149], 2244: [130], 2248: [113], 2261: [622], 2272: [386], 2274: [145, 239], 2276: [45, 49, 78, 86], 2284: [487], 2314: [21], 2323: [44], 2339: [118], 2343: [187], 2348: [75], 2350: [232, 770], 2353: [70], 2355: [350, 582], 2364: [461], 2369: [116], 2370: [192], 2371: [169], 2373: [52], 2386: [253], 2399: [121], 2408: [454], 2448: [705], 2459: [248], 2461: [27], 2489: [102], 2500: [459, 1640], 2503: [90, 205], 2509: [309], 2517: [994], 2531: [51], 2533: [53, 64], 2555: [165, 180, 536], 2578: [59], 2582: [347], 2587: [298], 2609: [123], 2611: [63], 2664: [170], 2690: [168], 2692: [890], 2695: [128], 2700: [286], 2702: [472], 2704: [119], 2710: [457], 2720: [178], 2731: [127], 2759: [940], 2781: [89], 2795: [17], 2804: [266], 2842: [38], 2850: [23, 30, 96], 2861: [28, 36], 2865: [59], 2879: [216, 364], 2930: [53, 270], 2934: [638, 864, 865], 2943: [22], 2955: [138, 172], 2960: [30], 2990: [90]}\n",
      "\n",
      "wish\n",
      " {0: [9], 10: [160], 16: [832], 34: [197], 59: [4], 60: [604], 65: [362], 69: [655], 76: [455], 95: [185], 121: [484], 124: [128], 128: [12], 135: [23], 143: [407], 171: [813], 189: [246, 323, 327, 337], 191: [64, 743, 758], 193: [415], 202: [352, 374, 377], 208: [103], 213: [127, 145], 218: [4], 241: [96], 250: [67, 565], 264: [166], 269: [86, 239], 293: [453], 317: [113, 115], 318: [182], 324: [116], 353: [764], 359: [79], 372: [160], 373: [5], 395: [578, 659], 401: [61], 405: [243], 412: [442, 997], 424: [59, 63], 427: [293], 435: [15], 438: [262], 449: [82], 464: [3, 13], 469: [38], 477: [35], 481: [170], 521: [154, 529, 1089, 1251, 1258], 531: [119], 537: [22], 542: [133], 547: [513], 582: [163, 181], 587: [130, 368], 600: [396, 432, 455, 474], 605: [66], 606: [142], 627: [479], 651: [433], 658: [93], 738: [180], 753: [43], 757: [50], 790: [160], 829: [96], 851: [1157, 1290], 878: [371], 888: [527], 915: [56], 920: [104], 927: [37, 53, 72, 331], 933: [57], 939: [315], 958: [75], 961: [78], 962: [43], 965: [47], 983: [329], 1001: [247], 1002: [80], 1033: [456], 1041: [190], 1081: [310], 1091: [152], 1095: [126], 1103: [32], 1122: [25], 1136: [397], 1148: [44], 1164: [62], 1175: [658], 1187: [149], 1197: [637, 749], 1217: [109], 1233: [34], 1263: [534], 1277: [1386, 2454], 1306: [244], 1339: [379], 1341: [64, 70], 1351: [14, 427], 1359: [188], 1435: [61], 1505: [60], 1507: [175], 1524: [103], 1528: [522], 1558: [316], 1577: [64], 1600: [250], 1617: [31], 1618: [393], 1634: [818, 1932, 2267], 1656: [478], 1658: [82], 1663: [409], 1667: [251], 1683: [217], 1696: [326], 1705: [469, 664, 1152, 1374], 1711: [215], 1732: [49], 1763: [901], 1781: [178], 1788: [458], 1798: [193], 1817: [1254], 1842: [42], 1857: [645, 667], 1862: [14], 1885: [19], 1890: [237], 1913: [584], 1922: [61], 1966: [343], 1977: [43], 1983: [676], 1999: [358], 2054: [1522, 1535, 1697, 1811, 1860], 2060: [231, 324], 2069: [322], 2089: [481], 2093: [69], 2109: [85, 162], 2120: [456], 2137: [65], 2144: [243], 2149: [323], 2150: [20], 2162: [974], 2181: [104], 2201: [151], 2213: [183], 2219: [323], 2221: [87], 2225: [34], 2236: [29, 704, 715], 2247: [196], 2261: [494], 2310: [26], 2358: [10], 2368: [6], 2418: [39, 129], 2471: [154], 2509: [377], 2522: [39], 2540: [432], 2549: [145], 2582: [141], 2601: [665], 2617: [66], 2665: [41], 2668: [399], 2714: [175], 2715: [140], 2769: [183], 2785: [216], 2795: [131], 2796: [163], 2809: [276], 2838: [127], 2849: [271], 2858: [262], 2886: [488], 2888: [4], 2897: [128], 2904: [267, 307], 2945: [409, 822], 2957: [101], 2964: [245], 2977: [104], 2996: [44]}\n"
     ]
    }
   ],
   "source": [
    "positional_index=build_positional_index(df_lemmaztized[\"Summary\"])\n",
    "print(\"Positional Index of first 10 terms in the vocabulary : \\n\")\n",
    "for i in list(positional_index.keys())[:10]:\n",
    "    print(f\"\\n{i}\\n {positional_index[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY :  chased police\n",
      "Query after preprocessing :  ['chased', 'police']\n",
      "Retreived Documents (Higer precedence to lower):  [2458, 83, 2691, 2476, 1434, 553, 2904, 345, 2367, 2054, 2406, 1357, 1234, 2804]\n",
      "Number of documents retreived :  14\n"
     ]
    }
   ],
   "source": [
    "query=\"chased police\"\n",
    "query_lemmatized,query_stemmed=preprocess_query_positional_index(query)\n",
    "retreived_documents_positional_index=search(positional_index,query_lemmatized)\n",
    "\n",
    "print(\"QUERY : \",query)\n",
    "print(\"Query after preprocessing : \",query_lemmatized)\n",
    "print(\"Retreived Documents (Higer precedence to lower): \",retreived_documents_positional_index)\n",
    "print(\"Number of documents retreived : \",len(retreived_documents_positional_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Proximity Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented by modifying Intersect algorithm For Proximity Constraint K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query_proximity_search(query):\n",
    "    query_terms=query.split(' ')\n",
    "    term1=query_terms[0]\n",
    "    term2=query_terms[2]\n",
    "    k=query_terms[1][1:]\n",
    "    return term1,term2,k\n",
    "\n",
    "# preprocess_query_proximity_search(\"rise /4 halfway\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximity_search(query,positional_index):\n",
    "    term1,term2,k=preprocess_query_proximity_search(query)\n",
    "    k=int(k)\n",
    "#     print(term1,term2,k)\n",
    "    if (term1 not in positional_index) or (term2 not in positional_index):\n",
    "        return \"No documents retreived (Token not found in corpus)\"\n",
    "    pos1_index=positional_index[term1]\n",
    "    pos2_index=positional_index[term2]\n",
    "    ret_docs=[]\n",
    "    doc_ids=[x for x in pos1_index if x in pos2_index]\n",
    "    for i in doc_ids:\n",
    "        positions=positional_index[term1][i]\n",
    "        for j in positional_index[term2][i]:\n",
    "            for x in positions:\n",
    "                if ((j-x<=k) and i not in ret_docs):      #words appearing with k places of each other\n",
    "                    ret_docs.append(i)\n",
    "    return ret_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query :  extra /100 work\n",
      "Retreived Documents:  [170, 498, 558, 1053, 1177, 1336, 1455, 1530, 1871, 2054, 2422]\n"
     ]
    }
   ],
   "source": [
    "query=\"extra /100 work\"\n",
    "print(\"Query : \",query)\n",
    "print(\"Retreived Documents: \",proximity_search(query,positional_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wildcard Queries\n",
    "&nbsp; 1. WILDCARD QUERIES on Inverted Index  \n",
    "&nbsp; 2. WILDCARD QUERIES on Positional Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WILDCARD QUERIES on Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wildcard queries on inverted index\n",
    "import re\n",
    "def wildcard_entry_inverted_index(query, inverted_index_dict):\n",
    "    retreived_docs=set()\n",
    "    matching_terms=[]\n",
    "    if query.endswith('*'):                     #leading wildcard entries\n",
    "        query_terms = query.split('*')\n",
    "        # print(query_terms)\n",
    "        query_terms=query_terms[0]\n",
    "        for i in inverted_index_dict:\n",
    "            if (i.startswith(query_terms)):\n",
    "                retreived_docs.update(inverted_index_dict[i])\n",
    "                matching_terms.append(i)\n",
    "\n",
    "    elif query.startswith(\"*\"):                 #trailing wildcard entries\n",
    "        query_terms = query.split('*')\n",
    "        # print(query_terms)\n",
    "        query_terms=query_terms[1]\n",
    "        for i in inverted_index_dict:\n",
    "            if (i.endswith(query_terms)):\n",
    "                retreived_docs.update(inverted_index_dict[i])\n",
    "                matching_terms.append(i)\n",
    "    else :                                      #widcard in between\n",
    "        if \"*\" in query:\n",
    "            l=len(query)-1\n",
    "        else:\n",
    "            l=len(query)                    \n",
    "        query_terms = query.split('*')\n",
    "        # print(query_terms)\n",
    "        for i in inverted_index_lemmatized:\n",
    "            if (i.startswith(query_terms[0]) and i.endswith(query_terms[1]) and len(i)>=l):\n",
    "                retreived_docs.update(inverted_index_dict[i])\n",
    "                matching_terms.append(i)\n",
    "    return retreived_docs,matching_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leading wild card query (Ex. mon*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching documents :  {256, 2945, 2054, 2313, 267, 1551, 2959, 1430, 2583, 2714, 1435, 284, 1692, 2423, 1696, 2849, 674, 931, 1788, 2086, 1960, 168, 424, 43, 178, 2486, 313, 1081, 2236, 2109, 62, 2243, 200, 460, 2255, 1620, 87, 2647, 475, 1250, 2403, 1893, 1638, 2026, 2282, 2923, 106, 2416, 2293, 1015, 1532, 1150}\n",
      "Matching Terms :  ['row', 'rowena', 'rowe', 'rowan', 'rowing', 'rowley', 'rowed', 'rowboat', 'rowling', 'rowen']\n",
      "Number of matched documents :  52\n"
     ]
    }
   ],
   "source": [
    "query_term=\"row*\"\n",
    "matching_docs,matching_terms = wildcard_entry_inverted_index(query_term, inverted_index_lemmatized)\n",
    "print(\"Matching documents : \", matching_docs)\n",
    "print(\"Matching Terms : \",matching_terms)\n",
    "print(\"Number of matched documents : \",len(matching_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trailing wild card query (Ex. *mon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching documents: {2054, 2058, 2250, 812, 1006, 1137, 341}\n",
      "matching Terms :  ['hello', 'othello']\n",
      "Number of matched documents :  7\n"
     ]
    }
   ],
   "source": [
    "query_term = \"*hello\"\n",
    "matching_docs,matching_terms = wildcard_entry_inverted_index(query_term, inverted_index_lemmatized)\n",
    "print(\"Matching documents:\", matching_docs)\n",
    "print(\"matching Terms : \",matching_terms)\n",
    "print(\"Number of matched documents : \",len(matching_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Wildcard in-between letters Ex. hel*ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching documents: {1411, 1667, 2436, 1669, 1802, 1937, 19, 1813, 2968, 1945, 2585, 2843, 2718, 1700, 1445, 806, 2599, 2091, 941, 2990, 815, 433, 1973, 2999, 2744, 1082, 449, 1217, 1474, 2498, 1219, 1226, 203, 590, 2261, 2006, 2133, 2264, 1497, 2648, 1115, 1881, 1882, 734, 350, 1504, 1632, 1762, 483, 2536, 1769, 1644, 878, 880, 1520, 2546, 2168}\n",
      "matching Terms :  ['prominent', 'pronouncement', 'proponent', 'proficient']\n",
      "Number of matched documents :  57\n"
     ]
    }
   ],
   "source": [
    "query_term = \"pro*ent\"\n",
    "matching_docs,matching_terms= wildcard_entry_inverted_index(query_term, inverted_index_lemmatized)\n",
    "print(\"Matching documents:\", matching_docs)\n",
    "print(\"matching Terms : \",matching_terms)\n",
    "print(\"Number of matched documents : \",len(matching_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WILDCARD QUERIES on Positional Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wildcard_entry_positionl_index(query,positional_index):\n",
    "    retreived_documents=set()\n",
    "    matching_terms=[]\n",
    "    if query.endswith(\"*\"):\n",
    "        query_terms=query.split(\"*\")\n",
    "        query_terms=query_terms[0]\n",
    "        for i in positional_index.keys():\n",
    "            if (i.startswith(query_terms)):\n",
    "                retreived_documents.update(positional_index[i])\n",
    "                matching_terms.append(i)\n",
    "    elif query.startswith(\"*\"):\n",
    "        query_terms=query.split(\"*\")\n",
    "        query_terms=query_terms[1]\n",
    "        for i in positional_index.keys():\n",
    "            if (i.endswith(query_terms)):\n",
    "                retreived_documents.update(positional_index[i])\n",
    "                matching_terms.append(i)\n",
    "    else:\n",
    "        if \"*\" in query:\n",
    "            l=len(query)-1\n",
    "        else:\n",
    "            l=len(query)\n",
    "        query_terms=query.split(\"*\")\n",
    "        for i in positional_index.keys():\n",
    "            if (i.startswith(query_terms[0]) and i.endswith(query_terms[1]) and len(i)):\n",
    "                retreived_documents.update(positional_index[i])\n",
    "                matching_terms.append(i)\n",
    "    wildcard_positional_index_match={}\n",
    "    for i in matching_terms:\n",
    "        wildcard_positional_index_match[i]=positional_index[i]\n",
    "    return retreived_documents,matching_terms,wildcard_positional_index_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leading wild card query (Ex. mon*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents retreived :  52\n",
      "\n",
      "Matching documents: {256, 2945, 2054, 2313, 267, 1551, 2959, 1430, 2583, 2714, 1435, 284, 1692, 2423, 1696, 2849, 674, 931, 1788, 2086, 1960, 168, 424, 43, 178, 2486, 313, 1081, 2236, 2109, 62, 2243, 200, 460, 2255, 1620, 87, 2647, 475, 1250, 2403, 1893, 1638, 2026, 2282, 2923, 106, 2416, 2293, 1015, 1532, 1150}\n",
      "\n",
      "Matching Terms :  ['row', 'rowena', 'rowe', 'rowan', 'rowing', 'rowley', 'rowed', 'rowboat', 'rowling', 'rowen']\n",
      "\n",
      "Positional index of matching terms :  {'row': {43: [55], 87: [279], 178: [18], 284: [94], 313: [18], 674: [172, 178], 1015: [59], 1250: [336], 1430: [180], 1435: [226], 1532: [239], 1893: [685], 1960: [60], 2026: [195], 2236: [593], 2243: [26], 2282: [30], 2313: [74], 2403: [323], 2583: [296], 2647: [281], 2714: [101], 2849: [790], 2923: [542]}, 'rowena': {62: [60, 78, 174, 273, 282, 293, 304, 322, 423], 256: [37], 1150: [3, 139, 141, 217, 331, 338, 532, 556, 593, 597, 614, 637, 677], 1696: [13, 186, 266, 397, 407, 420, 515, 617, 635, 761, 787, 858, 863], 2255: [94, 100, 142, 186]}, 'rowe': {106: [10]}, 'rowan': {168: [3, 37, 61, 74, 81, 98, 113, 148, 168, 196, 221, 256, 276, 316, 329, 331, 397], 200: [8], 267: [100, 116, 168, 182, 217, 225, 266, 307, 331, 363], 460: [4, 8, 27, 74, 102], 475: [7, 26, 30, 60], 931: [160], 2086: [49, 82, 126, 134, 282, 356, 363, 503, 512], 2109: [6, 71, 128, 132, 160, 171, 208, 248, 269, 277, 310, 318], 2293: [6, 16, 37, 80, 93, 125, 138, 146, 148, 262, 270, 280, 286], 2423: [31, 97, 124, 131, 132, 219]}, 'rowing': {424: [52], 1638: [214], 1960: [62], 2486: [10], 2945: [380]}, 'rowley': {1081: [21, 112, 121, 240, 244, 309, 314, 324, 336, 341, 355, 372, 384, 507, 523, 526]}, 'rowed': {1551: [204], 1620: [59]}, 'rowboat': {1692: [63, 99], 1788: [46, 76], 2054: [2243]}, 'rowling': {2416: [273]}, 'rowen': {2959: [95, 225, 241, 314]}}\n"
     ]
    }
   ],
   "source": [
    "query=\"row*\"\n",
    "retreived_documents,matching_terms,wildcard_positional_index_match=wildcard_entry_positionl_index(query,positional_index)\n",
    "print(\"Number of documents retreived : \",len(retreived_documents))\n",
    "print(\"\\nMatching documents:\", retreived_documents)\n",
    "print(\"\\nMatching Terms : \",matching_terms)\n",
    "print(\"\\nPositional index of matching terms : \",wildcard_positional_index_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trailing wild card query (Ex. *mon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents retreived :  7\n",
      "Matching documents: {1137, 341, 2054, 2250, 2058, 812, 1006}\n",
      "Matching Terms :  ['hello', 'othello']\n",
      "Positional index of matching terms :  {'hello': {341: [951], 1006: [157], 2054: [2711], 2058: [593, 600]}, 'othello': {812: [880], 1137: [40], 2250: [319]}}\n"
     ]
    }
   ],
   "source": [
    "query=\"*hello\"\n",
    "retreived_documents,matching_terms,wildcard_positional_index_match=wildcard_entry_positionl_index(query,positional_index)\n",
    "print(\"Number of documents retreived : \",len(retreived_documents))\n",
    "print(\"Matching documents:\", retreived_documents)\n",
    "print(\"Matching Terms : \",matching_terms)\n",
    "print(\"Positional index of matching terms : \",wildcard_positional_index_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Wildcard in-between letters Ex. hel*ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents retreived :  57\n",
      "\n",
      "Matching documents: {1411, 1667, 2436, 1669, 1802, 1937, 19, 1813, 2968, 1945, 2585, 2843, 2718, 1700, 1445, 806, 2599, 2091, 941, 2990, 815, 433, 1973, 2999, 2744, 1082, 449, 1217, 1474, 2498, 1219, 1226, 203, 590, 2261, 2006, 2133, 2264, 1497, 2648, 1115, 1881, 1882, 350, 734, 1504, 1632, 1762, 483, 2536, 1769, 1644, 878, 880, 1520, 2546, 2168}\n",
      "\n",
      "Matching Terms :  ['prominent', 'pronouncement', 'proponent', 'proficient']\n",
      "\n",
      "Positional index of matching terms :  {'prominent': {19: [213], 203: [40], 350: [166], 433: [5], 449: [176], 483: [764], 590: [408], 734: [491], 806: [77], 815: [363], 878: [397], 880: [150], 941: [151], 1082: [45], 1115: [128], 1217: [74], 1226: [32], 1411: [303], 1445: [130], 1474: [10], 1497: [22], 1504: [40], 1520: [12, 300], 1632: [15], 1644: [36], 1667: [37], 1700: [756], 1762: [178], 1769: [403], 1802: [228], 1813: [22], 1937: [81], 1945: [186], 1973: [51], 2006: [155], 2091: [106], 2168: [327], 2261: [485], 2264: [128], 2436: [337], 2498: [179], 2536: [39], 2585: [29], 2599: [33], 2648: [52], 2718: [334], 2843: [2], 2968: [209], 2990: [158], 2999: [19]}, 'pronouncement': {1219: [117], 2133: [286]}, 'proponent': {1632: [110], 1881: [197], 2744: [242]}, 'proficient': {1669: [102], 1882: [16], 2546: [87]}}\n",
      "\n",
      "Number of retreived documents :  57\n"
     ]
    }
   ],
   "source": [
    "query=\"pro*ent\"\n",
    "retreived_documents,matching_terms,wildcard_positional_index_match=wildcard_entry_positionl_index(query,positional_index)\n",
    "print(\"Number of documents retreived : \",len(retreived_documents))\n",
    "print(\"\\nMatching documents:\", retreived_documents)\n",
    "print(\"\\nMatching Terms : \",matching_terms)\n",
    "print(\"\\nPositional index of matching terms : \",wildcard_positional_index_match)\n",
    "print(\"\\nNumber of retreived documents : \",len(retreived_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compound Wild Card Query (Ex se\\*ate AND fil\\*er)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infix_to_postfix_wild_card(expr):\n",
    "    prec = {\"not\": 3, \"and\": 2, \"or\": 1}        #operator precedence\n",
    "    output = []         #output stack\n",
    "    op_stack = []       #operator stack\n",
    "    tokens = expr\n",
    "    for token in tokens:\n",
    "        if type(token)!=type([]) and token in prec:\n",
    "            while op_stack and op_stack[-1] in prec and prec[op_stack[-1]] >= prec[token]:\n",
    "                output.append(op_stack.pop())\n",
    "            op_stack.append(token)\n",
    "        else:\n",
    "            output.append(token)\n",
    "    while op_stack:\n",
    "        output.append(op_stack.pop())\n",
    "    return output\n",
    "\n",
    "def eval_postfix(lst):\n",
    "    stack = []\n",
    "    for token in lst:\n",
    "        if isinstance(token, list):\n",
    "            # If the token is an operand, push it onto the stack\n",
    "            stack.append(token)\n",
    "        elif token == \"not\":\n",
    "            # If the token is \"not\", pop the top operand from the stack, negate it, and push the result back onto the stack\n",
    "            operand = stack.pop()\n",
    "            # result = [x for x in range(len(df))]-operand\n",
    "            corpus=[x for x in range(3000)]\n",
    "            result=[x for x in corpus if x not in operand]\n",
    "            result.sort()\n",
    "            print(result)\n",
    "            stack.append(result)\n",
    "        elif token == \"and\":\n",
    "            # If the token is \"and\", pop the top two operands from the stack, perform a logical \"and\" operation, and push the result back onto the stack\n",
    "            operand2 = stack.pop()\n",
    "            operand1 = stack.pop()\n",
    "            # print(\"Operands : \",operand1,operand2)\n",
    "            result = [value for value in operand1 if value in operand2]\n",
    "            result.sort()\n",
    "            stack.append(result)\n",
    "        elif token == \"or\":\n",
    "            # If the token is \"or\", pop the top two operands from the stack, perform a logical \"or\" operation, and push the result back onto the stack\n",
    "            operand2 = stack.pop()\n",
    "            operand1 = stack.pop()\n",
    "            result = list(set(operand1) | set(operand2))\n",
    "            result.sort()\n",
    "            stack.append(result)\n",
    "        else:\n",
    "            # If the token is an unknown operator or operand, raise an error\n",
    "            raise ValueError(f\"Unknown token {token}\")\n",
    "    # The final result is the only element left on the stack\n",
    "    return stack.pop()\n",
    "\n",
    "def compound_wild_card_query(query):\n",
    "    query=query.lower()\n",
    "    query_terms=query.split()\n",
    "    terms=[]\n",
    "    docs=list()\n",
    "    for i in query_terms:\n",
    "        if \"*\" in i:\n",
    "            w,m=wildcard_entry_inverted_index(i,inverted_index_lemmatized)\n",
    "            terms.append(m)\n",
    "            d=set()\n",
    "            for i in m:\n",
    "                d.update(inverted_index_lemmatized[i])\n",
    "            d=list(d)\n",
    "            d.sort()\n",
    "            docs.append(d)\n",
    "        else:\n",
    "            terms.append(i)\n",
    "            docs.append(i)\n",
    "    postfix_expr=infix_to_postfix_wild_card(docs)\n",
    "#     print(\"Postfix Expression : \",postfix_expr)\n",
    "    ret=eval_postfix(postfix_expr)\n",
    "    return ret,terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retreived Documents :  [19, 203, 350, 433, 449, 483, 590, 734, 806, 815, 878, 880, 941, 1082, 1115, 1217, 1219, 1226, 1411, 1445, 1474, 1497, 1504, 1520, 1632, 1644, 1667, 1669, 1700, 1762, 1769, 1802, 1813, 1881, 1882, 1937, 1945, 1973, 2006, 2091, 2133, 2168, 2261, 2264, 2436, 2498, 2536, 2546, 2585, 2599, 2648, 2718, 2744, 2843, 2968, 2990, 2999]\n",
      "\n",
      "Total Documents Retreived :  57\n",
      "\n",
      "Matched Terms :  [['separate', 'senate', 'sedate', 'segregate'], 'and', ['filler', 'filibuster', 'filter'], 'or', ['prominent', 'pronouncement', 'proponent', 'proficient']]\n"
     ]
    }
   ],
   "source": [
    "retreived_documents,matched_terms=compound_wild_card_query(\"se*ate and fil*er or pro*ent\")\n",
    "print(\"Retreived Documents : \",retreived_documents)\n",
    "print(\"\\nTotal Documents Retreived : \",len(retreived_documents))\n",
    "print(\"\\nMatched Terms : \",matched_terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve relevant text using similarity index\n",
    "    1.Using Cosine Similarity\n",
    "    2.Using Jaccard Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TF-IDF vectorizer to embedd documents and query  \n",
    "Using Cosine Similarity to find documents closest to the query and returning them in sorted order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_cosine_similaity(df_preprocessed,query):\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "        corpus=df_preprocessed[\"Summary\"].tolist()\n",
    "        corpus_query=corpus+[query]\n",
    "\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(corpus_query)\n",
    "        features=tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "        cosine_similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "        return cosine_similarities,tfidf_matrix\n",
    "#Ranking the documents in decreasing order of cosine similarity score\n",
    "def df_sorted_cosine_similarities(cosine_similarities):\n",
    "    df_cosine_similarities={}\n",
    "    for i in range(len(df_preprocessed)):\n",
    "        df_cosine_similarities[i]=cosine_similarities[0][i]\n",
    "    sorted_cosine_similarities = dict(sorted(df_cosine_similarities.items(), key=lambda item: item[1],reverse= True))\n",
    "    return sorted_cosine_similarities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"point view mysterious stranger also known x renegade ethical contacting people valley enlisting aid posed engineer barry thorne airship\"\n",
    "cosine_similarities,tfidf_matrix=tfidf_cosine_similaity(df_preprocessed,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 documents retreived using Cosine Similarity: \n",
      "\n",
      "Doc ID\t\t Similarity Score\n",
      "666\t:\t0.19590542285762158\n",
      "2147\t:\t0.1434042771671043\n",
      "1589\t:\t0.1020069225459162\n",
      "2252\t:\t0.10074341836230337\n",
      "1635\t:\t0.07648369787196879\n",
      "389\t:\t0.07097635008855614\n",
      "323\t:\t0.06459727733156059\n",
      "1106\t:\t0.06293513909484898\n",
      "1050\t:\t0.06184385502785259\n",
      "928\t:\t0.061692437941195596\n"
     ]
    }
   ],
   "source": [
    "sorted_cosine_similarities=df_sorted_cosine_similarities(cosine_similarities)\n",
    "print(\"Top 10 documents retreived using Cosine Similarity: \")\n",
    "print(\"\\nDoc ID\\t\\t Similarity Score\")\n",
    "for i in list(sorted_cosine_similarities.keys())[:10]:\n",
    "    print(f\"{i}\\t:\\t{sorted_cosine_similarities[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Jaccard Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 documents retreived using Jaccard Similarity : \n",
      "\n",
      "Doc ID\t\tJaccard Similarity Score\n",
      "1635\t:\t0.06666666666666667\n",
      "666\t:\t0.06031746031746032\n",
      "2283\t:\t0.05555555555555555\n",
      "1852\t:\t0.05405405405405406\n",
      "1256\t:\t0.05\n",
      "2750\t:\t0.04918032786885246\n",
      "964\t:\t0.045454545454545456\n",
      "2920\t:\t0.0425531914893617\n",
      "2306\t:\t0.041666666666666664\n",
      "595\t:\t0.04081632653061224\n"
     ]
    }
   ],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union\n",
    "\n",
    "query=\"point view mysterious stranger also known x renegade ethical contacting people valley enlisting aid posed engineer barry thorne airship\"\n",
    "query_processed_lemmatized,query_processed_stemmed = preprocess_query_positional_index(query)\n",
    "\n",
    "n = len(df)\n",
    "jaccard_similarities = {}\n",
    "\n",
    "for i in range(n):\n",
    "    sim = jaccard_similarity(df_lemmaztized[\"Summary\"][i], query_processed_lemmatized)\n",
    "    if sim > 0:\n",
    "        jaccard_similarities[i] = sim\n",
    "\n",
    "sorted_jaccard_similarities = dict(sorted(jaccard_similarities.items(), key=lambda item: -item[1]))\n",
    "print(\"Top 10 documents retreived using Jaccard Similarity : \\n\")\n",
    "print(\"Doc ID\\t\\tJaccard Similarity Score\")\n",
    "for i in list(sorted_jaccard_similarities.keys())[:10]:\n",
    "    print(f\"{i}\\t:\\t{sorted_jaccard_similarities[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve relevant text using likelihood language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liklelihood language Model : Using Probabilistic Model on inverse-document-frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import string\n",
    " \n",
    "def likleihood_model_query_preprocessing(query):#lemmatization, stemming , lowercasing, punctuation\n",
    "    #lowercasing\n",
    "    query=query.lower()\n",
    "\n",
    "    #tokenization\n",
    "    query=wordpunct_tokenize(query)\n",
    "    \n",
    "    #removing punctuation\n",
    "    translator =str.maketrans('', '', string.punctuation+\" \")\n",
    "    query=[token.translate(translator) for token in query]\n",
    "    query=list(filter(None, query))\n",
    "\n",
    "    #stopword removal except and,or,not\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    query = [word for word in query if word not in stop_words]\n",
    "\n",
    "    #lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_query_terms=[lemmatizer.lemmatize(w) for w in query]\n",
    "\n",
    "    #stemming\n",
    "    ps=PorterStemmer()\n",
    "    stemmed_query_terms=[ps.stem(w) for w in query]\n",
    "\n",
    "    return lemmatized_query_terms,stemmed_query_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'book begin point view mysterious stranger also known x renegade ethical contacting people valley enlisting aid posed engineer barry thorne airship parseval murdered milton firebrass several others fellow ethicals posing mayan named ah qaaq traveling company chinese poet li po internal reverie reveals secret identity discovered monat grrautut director riverworld project monat recalled dark tower judged however used remote command kill inhabitant tower stopped resurrection agent valley could return tower stuck valley like others began make way upriver hoping catch ride one riverboats reverie end one traumatic day riverworld day left bank grailstones fail fire done meteorite sam clemens discovered broke circuit powered time damage repaired ethicals time either dead stuck valley one day grailstones fail inhabitant left bank invade right en masse half humanity dy conflict time richard burton friend joined crew king john ship rex grandissimus participate defense boat grailstones fail burton masquerading dark age british warrior eventually becomes king john security chief one day tom mix jack london peter jairus frigate apply join crew burton recognizing frigate attack realizing mistake burton eventually becomes friendly frigate becomes ally mix london false frigate disappeared travelling companion monat grrautut boarded sam clemens ship shortly boarding monat murdered renegade ethical eventually two riverboats reach virolando wide lake last inhabited stretch headwater also home pacifist church second chance hermann gring become priest king john clemens finally get square initially begin aerial dog fight plane destroyed two great riverboats attack ensuing conflict riverboats sunk crew die two captain killed king john killed clemens clemens dy heart attack pulled water mortal enemy erik bloodaxe clemens dy notice bloodaxe become adherent church second chance seek revenge clemens among survivor burton frigate alice kaz joe miller li po ah qaaq nur ed din joined american piano player tom turpin english novelist aphra behn man claiming gilgamesh french soldier jean marcelin baron de marbot take one ship launch craft survive fight proceed upriver scale waterfall end river make way much difficulty polar sea arrive tower unmask ah qaaq renegade ethical take prisoner explains identity loga son king priam troy like child died age five resurrected century ago raised alien ethicals one agent reveals also purpose riverworld meant grand moral test allow humanity progress point achieve enlightenment project brought close soul achieved enlightenment set loose wander universe aimlessly loga became obsessed sparing earthly family fate worked sabotage project one awakened burton prematurely recruited clemens others used satellite direct meteorite land near clemens revealed discover problem computer run tower part malfunctioned since one around repair problem reached critical point nothing done computer die releasing stored wathans soul people died riverworld without wathans people never brought back life addition monat locked user accessing computer gring followed group tower attempt fix computer killed security measure put place monat alice devise way get around programming deactivate security loga able repair computer'"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed[\"Summary\"][666]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID's of top 10 documents :\n",
      "Doc ID\t\tScore\n",
      "666\t\t 52.6687783299138\n",
      "521\t\t 19.877290499439464\n",
      "2692\t\t 16.430818481527506\n",
      "1688\t\t 15.59305293206993\n",
      "825\t\t 15.0394650328446\n",
      "341\t\t 14.704204183409683\n",
      "2517\t\t 14.305218041399229\n",
      "483\t\t 12.94578113444922\n",
      "569\t\t 12.94578113444922\n",
      "331\t\t 12.768805263980017\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "query = \"mysterious stranger also known x renegade ethical contacting people valley enlisting aid posed engineer\"\n",
    "query_lemmatized_terms,query_stemmed_terms = likleihood_model_query_preprocessing(query)\n",
    "# print(\"Processed Query : \",query_lemmatized_terms)\n",
    "\n",
    "\n",
    "# Calculate document frequencies\n",
    "doc_freq = {}\n",
    "for doc in df_lemmaztized[\"Summary\"]:\n",
    "    for word in set(doc):\n",
    "        if word in doc_freq:\n",
    "            doc_freq[word] += 1\n",
    "        else:\n",
    "            doc_freq[word] = 1\n",
    "\n",
    "# Calculate inverse document frequencies\n",
    "num_docs = len(df_lemmaztized)\n",
    "inv_doc_freq = {}\n",
    "for word in doc_freq:\n",
    "    inv_doc_freq[word] = math.log(num_docs / doc_freq[word])\n",
    "\n",
    "# Calculate likelihood scores for each document\n",
    "doc_scores = []\n",
    "for doc in df_lemmaztized[\"Summary\"]:\n",
    "    score = 0\n",
    "    for word in query_lemmatized_terms:\n",
    "        if word in doc:\n",
    "            score += inv_doc_freq[word]\n",
    "    doc_scores.append(score)\n",
    "\n",
    "# Sort documents by score and print top results\n",
    "results = sorted(zip(df_lemmaztized[\"Doc_ID\"], doc_scores), key=lambda x: x[1], reverse=True)\n",
    "print(\"Document ID's of top 10 documents :\")\n",
    "print(\"Doc ID\\t\\tScore\")\n",
    "for i in range(min(len(results), 10)):\n",
    "    print(f\"{results[i][0]}\\t\\t {results[i][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced search: \n",
    "    1.Relevance feedback  \n",
    "    2.Semantic matching  \n",
    "    3.Reranking of results  \n",
    "    4.Finding out query intention  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit Relevance feedback : Rocchio algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def rocchio_algorithm(query_vector, relevant_docs, non_relevant_docs, alpha=1, beta=0.75, gamma=0.15):\n",
    "    #alpha: weight of the initial query vector\n",
    "    #beta: weight of the relevant documents\n",
    "    #gamma: weight of the non-relevant documents\n",
    "    \n",
    "    relevant_centroid = np.mean(relevant_docs, axis=0)\n",
    "    irrelevant_centroid = np.mean(non_relevant_docs, axis=0)\n",
    "    updated_query_vector = alpha * query_vector + beta * relevant_centroid - gamma * irrelevant_centroid\n",
    "    return updated_query_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_query=\"point view mysterious stranger also known x renegade ethical contacting people valley enlisting aid posed engineer barry thorne airship\"\n",
    "cosine_similarities,tfidf_matrix=tfidf_cosine_similaity(df_preprocessed,initial_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix=tfidf_matrix.toarray()\n",
    "initial_query_tfidf=tfidf_matrix[-1]\n",
    "sorted_cosine_similarities_relevance=df_sorted_cosine_similarities(cosine_similarities)\n",
    "relevant_docs=[x for x in list(sorted_cosine_similarities_relevance.values())[:10]]\n",
    "non_relevant_docs=[x for x in list(sorted_cosine_similarities_relevance.values())[10:]]\n",
    "\n",
    "# print(len(relevant_docs),len(non_relevant_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_query_vector=rocchio_algorithm(initial_query_tfidf,relevant_docs, non_relevant_docs, alpha=1, beta=0.75, gamma=0.15)\n",
    "# next_query_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicit Relevance Feedback : Rocchio algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the query vector using the Rocchio algorithm\n",
    "def explicit_rocchio_algorithm(initial_query_tfidf,tfidf_matrix,relevant_docs,irrelevant_docs,alpha=1, beta=0.75, gamma=0.15):\n",
    "    relevant_vectors = tfidf_matrix[list(relevant_docs)]\n",
    "    irrelevant_vectors = tfidf_matrix[list(irrelevant_docs)]\n",
    "    if len(relevant_docs)==0:\n",
    "        relevant_mean=0\n",
    "    else:\n",
    "        relevant_mean = np.mean(relevant_vectors, axis=0)\n",
    "    if len(irrelevant_docs)==0:\n",
    "        irrelevant_mean=0\n",
    "    else:\n",
    "        irrelevant_mean = np.mean(irrelevant_vectors, axis=0)\n",
    "    \n",
    "    updated_query_vector = alpha * initial_query_tfidf + beta * relevant_mean - gamma * irrelevant_mean\n",
    "    \n",
    "    # Get new top-10 results\n",
    "    scores = cosine_similarity(np.reshape(updated_query_vector, (1, -1)), tfidf_matrix[:-1])[0]\n",
    "    updated_top_10 = np.argsort(scores)[::-1][:10]\n",
    "    return updated_query_vector,updated_top_10,scores\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_query=\"point view mysterious stranger also known x renegade ethical contacting people valley enlisting aid posed engineer barry thorne airship\"\n",
    "cosine_similarities,tfidf_matrix=tfidf_cosine_similaity(df_preprocessed,initial_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix=tfidf_matrix.toarray()\n",
    "initial_query_tfidf=tfidf_matrix[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_cosine_similarities_relevance=df_sorted_cosine_similarities(cosine_similarities)\n",
    "top_10_relevant_docs={}\n",
    "for doc_id in list(sorted_cosine_similarities_relevance.keys())[:10]:\n",
    "    top_10_relevant_docs[doc_id]=sorted_cosine_similarities_relevance[doc_id]\n",
    "\n",
    "# book_title=df[\"Book_Name\"][top_10_relevant_docs]\n",
    "# book_summary=df[\"Summary\"][top_10_relevant_docs]\n",
    "\n",
    "# print(\"Doc ID \\tSimilarity Score\\tBook Title\\t\\t\\tSummary\")\n",
    "# for i, doc_id in enumerate(top_10_relevant_docs):\n",
    "#     book_title=df[\"Book_Name\"][doc_id]\n",
    "#     book_summary=df[\"Summary\"][doc_id]\n",
    "#     print(f\"{doc_id}\\t{top_10_relevant_docs[doc_id]}\\t{book_title}\\t{book_summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc ID</th>\n",
       "      <th>Score</th>\n",
       "      <th>Book Title</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>666</td>\n",
       "      <td>0.195905</td>\n",
       "      <td>The Magic Labyrinth</td>\n",
       "      <td>The book begins from the point of view of The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2147</td>\n",
       "      <td>0.143404</td>\n",
       "      <td>I Know What You Did Last Summer</td>\n",
       "      <td>In an unnamed town, high school senior Julie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1589</td>\n",
       "      <td>0.102007</td>\n",
       "      <td>The Phoenix</td>\n",
       "      <td>Fact and fiction are combined to tell the sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2252</td>\n",
       "      <td>0.100743</td>\n",
       "      <td>Ghoul</td>\n",
       "      <td>Timmy Graco and his best friends Barry Smetzl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1635</td>\n",
       "      <td>0.076484</td>\n",
       "      <td>The Glass Books of the Dream Eaters</td>\n",
       "      <td>The book follows three main characters, Miss ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>389</td>\n",
       "      <td>0.070976</td>\n",
       "      <td>In the Empire of Shadow</td>\n",
       "      <td>A group from this world is trapped in a scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>323</td>\n",
       "      <td>0.064597</td>\n",
       "      <td>Ilse Witch</td>\n",
       "      <td>It has been 130 years since the events of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1106</td>\n",
       "      <td>0.062935</td>\n",
       "      <td>Chickenfeed</td>\n",
       "      <td>Based on the real life case of Elsie Cameron,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1050</td>\n",
       "      <td>0.061844</td>\n",
       "      <td>The Angel of Darkness</td>\n",
       "      <td>Stevie Taggert tells of the mystery of Seora...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>928</td>\n",
       "      <td>0.061692</td>\n",
       "      <td>Mortal Engines</td>\n",
       "      <td>The main character of Mortal Engines is Tom N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Doc ID     Score                           Book Title  \\\n",
       "0     666  0.195905                  The Magic Labyrinth   \n",
       "1    2147  0.143404      I Know What You Did Last Summer   \n",
       "2    1589  0.102007                          The Phoenix   \n",
       "3    2252  0.100743                                Ghoul   \n",
       "4    1635  0.076484  The Glass Books of the Dream Eaters   \n",
       "5     389  0.070976              In the Empire of Shadow   \n",
       "6     323  0.064597                           Ilse Witch   \n",
       "7    1106  0.062935                          Chickenfeed   \n",
       "8    1050  0.061844                The Angel of Darkness   \n",
       "9     928  0.061692                       Mortal Engines   \n",
       "\n",
       "                                             Summary  \n",
       "0   The book begins from the point of view of The...  \n",
       "1   In an unnamed town, high school senior Julie ...  \n",
       "2   Fact and fiction are combined to tell the sto...  \n",
       "3   Timmy Graco and his best friends Barry Smetzl...  \n",
       "4   The book follows three main characters, Miss ...  \n",
       "5   A group from this world is trapped in a scien...  \n",
       "6   It has been 130 years since the events of the...  \n",
       "7   Based on the real life case of Elsie Cameron,...  \n",
       "8   Stevie Taggert tells of the mystery of Seora...  \n",
       "9   The main character of Mortal Engines is Tom N...  "
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.options.display.width=None\n",
    "df_roccio=pd.DataFrame(list(zip(top_10_relevant_docs.keys(),top_10_relevant_docs.values(),df[\"Book_Name\"][top_10_relevant_docs],df[\"Summary\"][top_10_relevant_docs])),columns=[\"Doc ID\",\"Score\",\"Book Title\",\"Summary\"])\n",
    "df_roccio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to give feedback ? (yes/no)yes\n",
      "Enter 'r' for relevant, 'i' for irrelevant, or 'q' to quit: r\n",
      "Enter the relevant document number: 666\n",
      "Enter 'r' for relevant, 'i' for irrelevant, or 'q' to quit: r\n",
      "Enter the relevant document number: 1589\n",
      "Enter 'r' for relevant, 'i' for irrelevant, or 'q' to quit: r\n",
      "Enter the relevant document number: 2252\n",
      "Enter 'r' for relevant, 'i' for irrelevant, or 'q' to quit: i\n",
      "Enter the irrelevant document number: 389\n",
      "Enter 'r' for relevant, 'i' for irrelevant, or 'q' to quit: 323\n",
      "Enter 'r' for relevant, 'i' for irrelevant, or 'q' to quit: q\n",
      "\n",
      "Documents found relevant by the user :  [666, 2252, 1589] \n",
      "Documents found irrelevant by the user :  [389]\n",
      "   Doc ID     Score                           Book Title  \\\n",
      "0     666  0.011727                  The Magic Labyrinth   \n",
      "1    1589  0.017283      I Know What You Did Last Summer   \n",
      "2    2252  0.002344                          The Phoenix   \n",
      "3    2147  0.011075                                Ghoul   \n",
      "4     928  0.006432  The Glass Books of the Dream Eaters   \n",
      "5    1635  0.008125              In the Empire of Shadow   \n",
      "6     323  0.005537                           Ilse Witch   \n",
      "7     106  0.013799                          Chickenfeed   \n",
      "8    1050  0.011656                The Angel of Darkness   \n",
      "9     357  0.042890                       Mortal Engines   \n",
      "\n",
      "                                             Summary  \n",
      "0   The book begins from the point of view of The...  \n",
      "1   In an unnamed town, high school senior Julie ...  \n",
      "2   Fact and fiction are combined to tell the sto...  \n",
      "3   Timmy Graco and his best friends Barry Smetzl...  \n",
      "4   The book follows three main characters, Miss ...  \n",
      "5   A group from this world is trapped in a scien...  \n",
      "6   It has been 130 years since the events of the...  \n",
      "7   Based on the real life case of Elsie Cameron,...  \n",
      "8   Stevie Taggert tells of the mystery of Seora...  \n",
      "9   The main character of Mortal Engines is Tom N...  \n",
      "Do you want to give feedback ? (yes/no)no\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    update=input(\"Do you want to give feedback ? (yes/no)\")\n",
    "    update=update.lower()\n",
    "    relevant_docs = set()\n",
    "    irrelevant_docs = set()\n",
    "    if update==\"yes\":\n",
    "        while True:\n",
    "            feedback = input(\"Enter 'r' for relevant, 'i' for irrelevant, or 'q' to quit: \")\n",
    "            if feedback == \"r\":\n",
    "                doc_id = int(input(\"Enter the relevant document number: \"))\n",
    "                relevant_docs.add(doc_id)\n",
    "            elif feedback == \"i\":\n",
    "                doc_id = int(input(\"Enter the irrelevant document number: \"))\n",
    "                irrelevant_docs.add(doc_id)\n",
    "            elif feedback == \"q\":\n",
    "                break\n",
    "        \n",
    "        print(\"\\nDocuments found relevant by the user : \",list(relevant_docs),\"\\nDocuments found irrelevant by the user : \",list(irrelevant_docs))\n",
    "        updated_query_vector,updated_top_10,scores=explicit_rocchio_algorithm(initial_query_tfidf,tfidf_matrix,relevant_docs,irrelevant_docs,alpha=1, beta=0.75, gamma=0.15)\n",
    "        initial_query_tfidf=updated_query_vector\n",
    "        df_roccio=pd.DataFrame(list(zip(updated_top_10,scores,df[\"Book_Name\"][top_10_relevant_docs],df[\"Summary\"][top_10_relevant_docs])),columns=[\"Doc ID\",\"Score\",\"Book Title\",\"Summary\"])\n",
    "        print(df_roccio)\n",
    "    \n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps:** \n",
    " 1. Tokenize the query  \n",
    " 2. Find synonyms of the query and modify the query by replacing terms with their synonymns  \n",
    " 3. Modify each of the dicument by adding the synonyms of each of the terms  \n",
    " 4. Find cosine similarity between all modified documents and the modified query  \n",
    " 5. Rank documents based on similarity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    return list(set(synonyms))\n",
    "\n",
    "def get_relevant_documents(query, documents):\n",
    "    query_tokens = word_tokenize(query)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    query_lemmas = [lemmatizer.lemmatize(token) for token in query_tokens]\n",
    "    for i in range(len(query_lemmas)):\n",
    "        synonyms = get_synonyms(query_lemmas[i])\n",
    "        if len(synonyms) > 0:\n",
    "            query_lemmas[i] = '|'.join(synonyms)\n",
    "    modified_documents = []\n",
    "    for document in documents:\n",
    "        modified_document = []\n",
    "        for j in range(len(document)):\n",
    "            word = document[j]\n",
    "            synonyms = get_synonyms(word)\n",
    "            if len(synonyms) > 0:\n",
    "                modified_document.append(synonyms[0])\n",
    "            else:\n",
    "                modified_document.append(word)\n",
    "        modified_documents.append(modified_document)\n",
    "    v = []\n",
    "    for document in modified_documents:\n",
    "        vector = []\n",
    "        for lemma in query_lemmas:\n",
    "            vector.append(document.count(lemma))\n",
    "        v.append(vector)\n",
    "    query_vector = []\n",
    "    for lemma in query_lemmas:\n",
    "        query_vector.append(query_lemmas.count(lemma))\n",
    "    similarities = cosine_similarity(v, [query_vector])\n",
    "    sorted_documents = [documents[i] for i in np.argsort(similarities[:,0])[::-1]]\n",
    "    return sorted_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"point view mysterious stranger also known\"\n",
    "semantic_matched_docs = get_relevant_documents(query, df_lemmaztized[\"Summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant Documents based on semantic matching :\n",
      "Doc ID\t\tSummary\n",
      "0\t\tfirst chief henry lee novel open 1919 growing town delano georgia hire first police chief city council led banker prominent investor hugh holmes chooses farmer henry lee foxy funderburke eccentric wealthy dog breeder gun collector job henry unschooled policeman honest determined job well long assumes new responsibility dead body young man found naked bottom cliff medical examination concludes boy died broken neck result fall also tortured  cuffed beaten rubber hose  time death medical examiner tell henry crime strong sexual component boy yet sodomized assault could gone boy evidently escaped henry conduct thorough investigation frustrated attempt locate killer least uncooperative skeeter willis sheriff meriwhether county delano part insists boy killed many transient area time henry unconvinced eventually run lead belief second murder taking place four year later outside jurisdiction connected first real proof real authority pursue matter finally henry hears skeeter willis young runaway might passing near delano also learns someone delano po box attempted purchase pair handcuff police supplier box belongs foxy funderburke questioned two murder since body discovered near foxy property clue also point foxy direction looking around property henry notice outline appears freshly dug grave observed foxy rush obtain search warrant foxy follows intending kill nearby diverted problem involving cole black family worked shot killed black father grip malaria induced delirium henry dy shooter teenage son willie escaping relative another town second chief sonny butt scene shift 1946 world war ii delano welcome home returning veteran including billy lee late chief son young lawyer served bomber pilot europe sonny butt decorated army infantryman billy become protg hugh holmes two men decide launch billy politics run holmes seat georgia state senate banker intends retire sonny land position delano police force consists chief two officer quick success police officer rumor abuse police station heard chief dy sonny named replacement examining old police file sonny come across chief lee note two murder 1920s also begin track last known sighting number young men likely runaway year since notice geographical pattern disappearance delano center seeing henry note reference interviewing foxy funderburke sonny decides keep eye foxy possible suspect sonny undone virulent racism propensity violence beat kill local black businessman grand jury empaneled purpose indicting though perjured testimony let hook sonny also observed beating man county fair evening hugh holmes determined fire desperate hang onto job sonny head foxy property catch foxy digging fresh grave recent victim momentarily distracted chance save reputation sonny attacked foxy shoot buries new grave police motorcycle third chief tucker watt time frame change time 1963 billy lee georgia lieutenant governor planning run governor next election hugh holmes asks billy help searching new chief supervise six officer integrationist billy alerted resume major tucker watt retiring army mp black mindful delano yet hire black officer let alone chief billy decides forward watt resume holmes recommends hiring holmes make persuasive case watt city council hearing hired chief setting medium frenzy first black hired head police force south tucker assumes duty chief welcomed many resident scorned others make good impression overall community however first seems able weather problem face tucker harbor painful secret willie cole whose father killed chief lee decade earlier far delano know died accident long fleeing tucker deathly afraid discovered nearly kill man recognizes coming sens relaxes like sonny butt begin organize old police file read chief lee report old murder observes sonny butt notation regarding pattern subsequent disappearance building work two predecessor tucker also begin suspect aged foxy funderburke make inquiry confluence event distracts tucker including arrest trumped charge billy lee race governor tucker hiring becomes issue problem leave inclined nothing foxy time john howell reporter new york time convinces tucker take evidence fbi obtain search warrant fbi reluctantly agrees issue warrant tucker federal agent search foxy property finding nothing first tucker fearing might dealt fatal blow billy lee campaign one agent trip prof motorcycle handlebar protruding ground foxy emerges house armed intending kill tucker agent distracted john howell shot dead ensuing investigation determines foxy tortured sodomized murdered buried least forty three young men decade john howell discovers tucker true identity decides keep secret novel end hugh holmes pleased billy success election crushed revelation foxy shame murder bring city much build suffering potentially fatal heart attack\n",
      "1\t\tnovel concern people transported future totalitarian philadelphia 2118 inhaling grey dust\n",
      "2\t\tdetective chief inspector e morse thames valley police meet anne scott party hosted mr murdoch north oxford six month later anne scott found hanging kitchen 9 canal reach jericho oxford initially chief inspector bell closer oxford central station st aldate street assigned case fortnight later morse take investigation subsequently mr murdoch son edward ted murdoch michael murdoch well anne scott former employer brother charles richards conrad richards charles wife celia come attention morse m scott neighbour including nosy handyman george jackson sophocles oedipus rex latter also figure episode 3 1 spin tv series lewis\n",
      "3\t\tserge follows suitcase containing five million coast florida tampa proceeds meet lenny lipowicz part time johnson impersonator owns real moon rock 1970s space mission along ride city country two girl alabama run incident bar bathroom girl accidentally stabbed steak knife using snort cocaine one girl pulled blade trying help cut artery fingerprint decided run also gangster named fiddlebottom changed name menacing zargoza hemingway impersonator named jethro character meet end hammerhead ranch motel hurricane rolando berto come ashore\n",
      "4\t\tstaying aunt jane suffolk adam dalgliesh stumble across bizarre frightening murder local detective novelist maurice seton becomes subject investigation boat wash ashore body inside hand cut seemingly meat cleaver strangely scene death mirrored manuscript new thriller writing\n",
      "5\t\tstory told peter aaron victim benjamin sachs best friend first meet fellow writer greenwich village bar 1975 peter decides try piece together story ben life agent f b approach course investigation friendship peter acknowledges ben  lost year suffering painful inner state saying  15 year sachs travelled one end time came last place doubt even knew anymore much distance covered possible remember begun two first meet struggling novelist peter  wheeling  mind provocative ben perfect marriage beautiful fanny wish  say something  make difference real world privately ben full doubt marriage showing crack one night drunken party freakish chance tumble fourth floor fire escape nearly losing life fall actual metaphorical day afterward refuse speak recovery strangely remote within week turning 41 ben express desire end life lived feeling life waste declares want everything change serving nothing ultimatum decides must take control fail evincing change leaf fanny move cabin vermont begin work book  vanishes contact fanny one final meeting peter confesses cabin content deserted manuscript titled leviathan lie abandoned peter piece together ben  life disappearing involves photographic artist maria closest friend lillian lillian 39 husband vietnam war veteran reed becomes ben  alter ego violent random encounter sends ben radically new direction\n",
      "6\t\tjane marple elderly amateur sleuth take holiday bertram hotel london live happy memory staying youth hotel famous fully preserving edwardian atmosphere even 1960s proper staff elderly guest frequent tearoom miss marple first see lady selina hazy childhood friend lady hazy say often think recognizes people hotel turn stranger miss marple intrigued guest tearoom especially famous adventuress be sedgwick young woman elvira blake guardian colonel luscombe forgetful clergyman canon pennyfather elvira late father left lot money held trust yet 20 turn 21 mother be sedgwick abandoned toddler become famous star adventurer kept touch elvira suddenly start asking guardian would inherit money dy hint may planning marriage also say somebody tried poison school day italy secretly fly ireland 24 hour telling best friend find something terrible importance canon pennyfather supposed fly switzerland day also 24 hour attend religious conference lucerne forgetful arrive airport following evening time conference return hotel around midnight upon entering room see something surprising immediately knocked head wake four day later house several hour london near location irish mail train robbed three day earlier family found side road taken remembers nothing since taking taxi airport yet witness train robbery say saw somebody looked like scene miss marple also saw leaving hotel room 3 three hour knocked head hour robbery turn lady sedgwick hidden elvira consider suitable mother given lifestyle sedgwick elvira lover man racing car driver ladislaus malinowski however woman claim elvira know miss marple know seen elvira malinowski together restaurant think malinowski unsuitable man elvira wish could save getting involved meanwhile car similar malinowski seen train robbery several train robbery similar identical licence plate one digit miss marple overhears be sedgwick talking hotel commissionaire micky gorman turn earlier married ireland time gorman told wedding game legal marriage fact real marriage four subsequent marriage unwittingly bigamous elvira also overhears worry might invalidate inheritance daughter one sedgwick later husband travelled ireland verify marriage know whether flew back england took train perhaps irish mail train could witness perpetrator robbery elvira come hotel one foggy night two shot fired elvira found next dead commissionaire gorman elvira claim shot dead run front shield due first shot gun malinowski police chief inspector father davy along inspector campbell involved mystery since pennyfather disappearance interview everybody hotel quickly realizes miss marple notice thing  thing human nature provide important clue pennyfather found three try experiment miss marple pennyfather enact action saw hallway although remember realizes saw walk different pennyfather remembers surprised entered room saw sitting chair knocked head doppelgnger confederate left hotel miss marple saw drove unconscious pennyfather mail train made visible robbery people would mistake pennyfather left pennyfather side road miss marple tell inspector davy disappointed find much hotel edwardian atmosphere false guest genuine others actor pretending people lady hazy wrong people mistakenly recognized actor pretending people knew hotel would many actor baffling sleuth realize hotel center criminal ring actor pose people robbery order make look like namesake place father davy miss marple confront be sedgwick orchestrator robbery along hotel owner staff sedgwick confesses also admits killing gorman drive away recklessly commits suicide although racing may let look like accident however miss marple already concluded elvira shot gorman mother falsely confessed shooting order save daughter elvira fallen madly love ladislaus malinowski knew primarily interested money concerned michael gorman revealed marriage be sedgwick would endanger father legacy\n",
      "7\t\tnovel central character joe cashin melbourne homicide detective following serious physical injury posted hometown begin process rebuilding old family mansion physical mental strength background family tragedy politics police corruption racism investigates death wealthy local man charles burgoyne closest friend police superior villani central character truth\n",
      "8\t\tsakkara superpower research facility heart united state adolescent superhumans quantum prophecy return time cover blown forced flee u order protect attack publicity facility hide thought secret name known around world following terrorist attack supervillain turned assassin leaf word sakkara spraypainted wall airport killing dozen people someone among new hero old hero broken protocol everyone suspect attack begin occur pattern emerges going trutopians trutopians international organisation designed give member security equality reduced comfort freedom revealed run antagonist last novel victor cross really trying take world new hero learn power discover hero colin learns actually inherited power mom energy instead father titan hero learn yvonne one leaking information end solomon cord paragon dy decision made colin\n",
      "9\t\tnovel take place 25 year event previous novel skyfall eldrinson valdoria wife roca skolia live happily homeworld lyshriol ten child already left home like second oldest son althor training become jagernaut firstborn eldrin request skolian assembly marry aunt ruby pharaoh dyhianna selei sixth valdoria child 16 year old sauscony soz want enter military academy become jagernaut like brother eldrinson plan  would rather prefer see little girl living safely lyshriol married local landlord disobeys disowns leaving althor taking world though regret harsh word immediately chance take back soon teenage son shannon unhappy family discord run away home book told perspective several main character  young soz military training shannon searching lost kin mystic blue dale archer father eldrinson captured crippled nearly tortured death sadistic aristo infiltrated lyshriol destroy ruby dynasty\n"
     ]
    }
   ],
   "source": [
    "print(\"Relevant Documents based on semantic matching :\")\n",
    "print(\"Doc ID\\t\\tSummary\")\n",
    "for doc_id in range(len(semantic_matched_docs))[:10]:\n",
    "  print(f'{doc_id}\\t\\t{\" \".join(semantic_matched_docs[doc_id])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
